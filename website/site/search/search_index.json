{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Nova: Natural Language Interface for Deep Learning","text":"<p>Bridging the gap between intuitive language and PyTorch code</p> \ud83d\udd24 Natural Language <p>Express machine learning concepts in intuitive, human-readable language</p> \ud83d\udd04 Pipeline Approach <p>Conceptualize neural networks as familiar data transformation pipelines</p> \ud83e\udde0 Education-Focused <p>Learn PyTorch concepts through accessible terminology and examples</p> \u26a1 Productivity <p>Rapidly prototype machine learning models with minimal boilerplate</p>"},{"location":"#what-is-nova","title":"What is Nova?","text":"<p>Nova is a natural language interface that transforms how we approach deep learning. It allows developers, researchers, and students to express machine learning concepts using intuitive language, which gets translated into executable PyTorch code.</p> <p>Instead of wrestling with complex PyTorch syntax and concepts, Nova lets you focus on what your model should do, not how to code it.</p> <pre><code># Nova code\ncreate processing pipeline digit_recognizer:\n    add transformation stage fully_connected with 784 inputs and 128 outputs\n    apply relu activation\n    add transformation stage fully_connected with 128 inputs and 10 outputs\n</code></pre> <p>translates to:</p> <pre><code># PyTorch code\nclass DigitRecognizer(nn.Module):\ndef __init__(self):\nsuper(DigitRecognizer, self).__init__()\nself.fc1 = nn.Linear(784, 128)\nself.relu = nn.ReLU()\nself.fc2 = nn.Linear(128, 10)\ndef forward(self, x):\nx = self.fc1(x)\nx = self.relu(x)\nx = self.fc2(x)\nreturn x\n</code></pre>"},{"location":"#why-nova","title":"Why Nova?","text":"\ud83d\udeab No Jargon <p>Replace complex machine learning terminology with intuitive concepts that make sense to everyone</p> \ud83d\udd0d Transparency <p>See exactly how your natural language translates to PyTorch code</p> \ud83c\udf31 Gentle Learning Curve <p>Gradually learn PyTorch concepts through natural language and translations</p> \ud83e\udde9 Familiar Patterns <p>Use the pipeline metaphor familiar to most programmers</p> <p>Machine learning and deep learning have steep learning curves due to:</p> <ol> <li>Complex Terminology: Terms like tensors, gradients, and backpropagation can be intimidating</li> <li>Mathematical Foundations: Understanding the math can be challenging for many developers</li> <li>Framework-Specific Patterns: Learning PyTorch's specific implementation patterns takes time</li> </ol> <p>Nova addresses these challenges by providing an intuitive bridge between human understanding and technical implementation.</p>"},{"location":"#quick-example","title":"Quick Example","text":"Nova CodePyTorch Code <pre><code># Load data\nload data collection mnist from torchvision.datasets with:\n    apply normalization with mean 0.1307 and deviation 0.3081\n    convert to feature grid\n\n# Create model\ncreate processing pipeline digit_classifier:\n    add transformation stage fully_connected with 784 inputs and 256 outputs\n    apply relu activation\n    add transformation stage fully_connected with 256 inputs and 10 outputs\n\n# Train model\ntrain digit_classifier on mnist_dataloader:\n    measure error using cross_entropy\n    improve using adam with learning rate 0.001\n    repeat for 5 learning cycles\n</code></pre> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n# Load data\ntransform = transforms.Compose([\ntransforms.ToTensor(),\ntransforms.Normalize((0.1307,), (0.3081,))\n])\nmnist_dataset = datasets.MNIST('./data', download=True, transform=transform)\nmnist_dataloader = DataLoader(mnist_dataset, batch_size=64, shuffle=True)\n# Create model\nclass DigitClassifier(nn.Module):\ndef __init__(self):\nsuper(DigitClassifier, self).__init__()\nself.fc1 = nn.Linear(784, 256)\nself.relu = nn.ReLU()\nself.fc2 = nn.Linear(256, 10)\ndef forward(self, x):\nx = x.view(-1, 784)  # Flatten the input\nx = self.fc1(x)\nx = self.relu(x)\nx = self.fc2(x)\nreturn x\nmodel = DigitClassifier()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n# Train model\nfor epoch in range(5):\nfor data, target in mnist_dataloader:\noptimizer.zero_grad()\noutput = model(data)\nloss = criterion(output, target)\nloss.backward()\noptimizer.step()\n</code></pre>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to try Nova? Follow our getting started guide to begin using natural language for deep learning.</p> \ud83d\udce5 Installation <p>Install Nova and set up your environment</p> \ud83d\ude80 Quick Start <p>Create your first model using Nova</p> \ud83d\udcda Language Guide <p>Learn the Nova language and syntax</p> \ud83d\udca1 Examples <p>Explore practical examples using Nova</p>"},{"location":"api/extensions/","title":"Nova Extensions API","text":"<p>The Nova extensions API allows you to extend the core functionality of Nova with custom components, operations, and integrations. This document covers how to create and use Nova extensions.</p>"},{"location":"api/extensions/#extension-types","title":"Extension Types","text":"<p>There are several ways to extend Nova:</p> <ol> <li>Custom Mappings: Simple mappings for new terminology</li> <li>Custom Components: New component classes for more complex behavior</li> <li>Plugins: Comprehensive extensions that can modify multiple aspects of Nova</li> <li>Framework Adapters: Support for translating to frameworks other than PyTorch</li> </ol>"},{"location":"api/extensions/#creating-custom-mappings","title":"Creating Custom Mappings","text":"<p>The simplest way to extend Nova is by adding custom mappings for new terminology.</p>"},{"location":"api/extensions/#error-measures-loss-functions","title":"Error Measures (Loss Functions)","text":"<pre><code>from nova import NovaInterpreter\n# Define custom mappings\ncustom_error_measures = {\n\"weighted_cross_entropy\": \"nn.CrossEntropyLoss(weight=torch.tensor([0.2, 0.8]))\",\n\"huber_loss\": \"nn.HuberLoss(delta=1.0)\",\n\"contrastive_loss\": \"ContrastiveLoss(margin=1.0)\"\n}\n# Create interpreter with custom mappings\ninterpreter = NovaInterpreter(\ncustom_mappings={\"error_measures\": custom_error_measures}\n)\n# Now you can use these in Nova code\nnova_code = \"\"\"\ntrain model on data_stream:\n    measure error using weighted_cross_entropy\n    improve using adam with learning rate 0.001\n    repeat for 10 learning cycles\n\"\"\"\npytorch_code = interpreter.translate(nova_code)\n</code></pre>"},{"location":"api/extensions/#improvement-strategies-optimizers","title":"Improvement Strategies (Optimizers)","text":"<pre><code>custom_optimizers = {\n\"adamw\": \"optim.AdamW\",\n\"radam\": \"optim.RAdam\",\n\"lookahead\": \"Lookahead\"\n}\ninterpreter = NovaInterpreter(\ncustom_mappings={\"improvement_strategies\": custom_optimizers}\n)\n</code></pre>"},{"location":"api/extensions/#activation-functions","title":"Activation Functions","text":"<pre><code>custom_activations = {\n\"swish\": \"nn.SiLU()\",\n\"mish\": \"nn.Mish()\",\n\"gelu\": \"nn.GELU()\"\n}\ninterpreter = NovaInterpreter(\ncustom_mappings={\"activations\": custom_activations}\n)\n</code></pre>"},{"location":"api/extensions/#creating-custom-components","title":"Creating Custom Components","text":"<p>For more complex extensions, you can create custom component classes.</p>"},{"location":"api/extensions/#component-base-classes","title":"Component Base Classes","text":"<p>Nova provides several base classes for components:</p> <ul> <li><code>ModelComponent</code>: Base for model definitions</li> <li><code>LayerComponent</code>: Base for layer definitions</li> <li><code>ActivationComponent</code>: Base for activation functions</li> <li><code>TrainingComponent</code>: Base for training operations</li> </ul>"},{"location":"api/extensions/#example-custom-layer-component","title":"Example: Custom Layer Component","text":"<pre><code>from nova.components import LayerComponent\nfrom nova.registry import register_component\nclass SelfAttentionComponent(LayerComponent):\ndef __init__(self, name, params):\nsuper().__init__(name, params)\nself.embed_dim = params.get('embed_dim', 512)\nself.num_heads = params.get('num_heads', 8)\ndef to_pytorch(self):\nreturn f\"nn.MultiheadAttention(embed_dim={self.embed_dim}, num_heads={self.num_heads})\"\ndef forward_code(self, input_var):\nreturn f\"attn_output, _ = self.{self.name}({input_var}, {input_var}, {input_var})\"\n# Register the component\nregister_component(\"self_attention\", SelfAttentionComponent)\n</code></pre> <p>Now you can use your custom component in Nova code:</p> <pre><code>create processing pipeline transformer:\n    add self_attention with embed_dim 512 and num_heads 8\n    add layer normalization with 512 features\n    add transformation stage fully_connected with 512 inputs and 2048 outputs\n    apply relu activation\n    add transformation stage fully_connected with 2048 inputs and 512 outputs\n    add layer normalization with 512 features\n</code></pre>"},{"location":"api/extensions/#example-custom-activation-component","title":"Example: Custom Activation Component","text":"<pre><code>from nova.components import ActivationComponent\nfrom nova.registry import register_component\nclass GELUComponent(ActivationComponent):\ndef __init__(self):\nsuper().__init__(\"gelu\")\ndef to_pytorch(self):\nreturn \"nn.GELU()\"\ndef forward_code(self, input_var):\nreturn f\"x = self.gelu({input_var})\"\n# Register the component\nregister_component(\"gelu_activation\", GELUComponent)\n</code></pre>"},{"location":"api/extensions/#creating-plugins","title":"Creating Plugins","text":"<p>Plugins provide a more structured way to extend Nova with multiple related components.</p>"},{"location":"api/extensions/#plugin-base-class","title":"Plugin Base Class","text":"<pre><code>from nova.plugin import NovaPlugin\nclass TransformerPlugin(NovaPlugin):\ndef __init__(self):\nsuper().__init__(\"transformer\")\ndef register(self, interpreter):\n# Register components\nfrom .components import (\nSelfAttentionComponent, \nLayerNormComponent,\nMultiHeadAttentionComponent\n)\ninterpreter.register_component(\"self_attention\", SelfAttentionComponent)\ninterpreter.register_component(\"layer_norm\", LayerNormComponent)\ninterpreter.register_component(\"multi_head_attention\", MultiHeadAttentionComponent)\n# Register mappings\ninterpreter.register_mapping(\"activations\", \"gelu\", \"nn.GELU()\")\ndef unregister(self, interpreter):\n# Clean up when plugin is unregistered\ninterpreter.unregister_component(\"self_attention\")\ninterpreter.unregister_component(\"layer_norm\")\ninterpreter.unregister_component(\"multi_head_attention\")\ninterpreter.unregister_mapping(\"activations\", \"gelu\")\n</code></pre>"},{"location":"api/extensions/#using-plugins","title":"Using Plugins","text":"<pre><code>from nova import NovaInterpreter\nfrom transformer_plugin import TransformerPlugin\n# Create interpreter\ninterpreter = NovaInterpreter()\n# Register plugin\ntransformer_plugin = TransformerPlugin()\ninterpreter.register_plugin(transformer_plugin)\n# Now you can use transformer components in Nova code\nnova_code = \"\"\"\ncreate processing pipeline transformer:\n    add self_attention with embed_dim 512 and num_heads 8\n    add layer_norm with 512 features\n\"\"\"\npytorch_code = interpreter.translate(nova_code)\n</code></pre>"},{"location":"api/extensions/#framework-adapters","title":"Framework Adapters","text":"<p>Nova can be extended to generate code for frameworks other than PyTorch.</p>"},{"location":"api/extensions/#creating-a-framework-adapter","title":"Creating a Framework Adapter","text":"<pre><code>from nova.adapters import FrameworkAdapter\nclass TensorFlowAdapter(FrameworkAdapter):\ndef __init__(self):\nsuper().__init__(\"tensorflow\")\ndef translate_imports(self, components):\nreturn \"import tensorflow as tf\\nfrom tensorflow import keras\\n\"\ndef translate_model(self, model_component):\n# Translate model definition to TensorFlow code\n# ...\ndef translate_layer(self, layer_component):\n# Translate layer to TensorFlow\nif layer_component.type == \"fully_connected\":\nreturn f\"keras.layers.Dense({layer_component.outputs})\"\n# ...\ndef translate_activation(self, activation_component):\n# Translate activation to TensorFlow\nif activation_component.name == \"relu\":\nreturn \"keras.activations.relu\"\n# ...\ndef translate_training(self, training_component):\n# Translate training code to TensorFlow\n# ...\n</code></pre>"},{"location":"api/extensions/#using-a-framework-adapter","title":"Using a Framework Adapter","text":"<pre><code>from nova import NovaInterpreter\nfrom tensorflow_adapter import TensorFlowAdapter\n# Create interpreter with TensorFlow adapter\ninterpreter = NovaInterpreter(adapter=TensorFlowAdapter())\n# Now Nova code will be translated to TensorFlow\nnova_code = \"\"\"\ncreate processing pipeline model:\n    add transformation stage fully_connected with 784 inputs and 128 outputs\n    apply relu activation\n    add transformation stage fully_connected with 128 inputs and 10 outputs\n\"\"\"\ntensorflow_code = interpreter.translate(nova_code)\n</code></pre>"},{"location":"api/extensions/#handling-special-syntax","title":"Handling Special Syntax","text":"<p>You can extend Nova's parser to handle special syntax for your custom components.</p>"},{"location":"api/extensions/#custom-parser-rules","title":"Custom Parser Rules","text":"<pre><code>from nova.parser import NovaParser, register_rule\nimport re\nclass CustomParser(NovaParser):\ndef __init__(self):\nsuper().__init__()\n@register_rule\ndef parse_attention(self, line, context):\npattern = r'add attention from (\\w+) to (\\w+) with (\\d+) heads'\nmatch = re.match(pattern, line.strip())\nif match:\nfrom_var, to_var, num_heads = match.groups()\nreturn {\n\"type\": \"attention\",\n\"from_var\": from_var,\n\"to_var\": to_var,\n\"num_heads\": int(num_heads)\n}\nreturn None\n# Use the custom parser\nfrom nova import NovaInterpreter\ninterpreter = NovaInterpreter(parser=CustomParser())\n</code></pre>"},{"location":"api/extensions/#creating-domain-specific-extensions","title":"Creating Domain-Specific Extensions","text":"<p>Nova can be extended with domain-specific knowledge for areas like computer vision, NLP, or reinforcement learning.</p>"},{"location":"api/extensions/#example-computer-vision-extension","title":"Example: Computer Vision Extension","text":"<pre><code>from nova.plugin import NovaPlugin\nclass ComputerVisionPlugin(NovaPlugin):\ndef __init__(self):\nsuper().__init__(\"computer_vision\")\ndef register(self, interpreter):\n# Register components for computer vision\nfrom .components import (\nResNetBlockComponent,\nInceptionBlockComponent,\nDepthwiseSeparableConvComponent\n)\ninterpreter.register_component(\"resnet_block\", ResNetBlockComponent)\ninterpreter.register_component(\"inception_block\", InceptionBlockComponent)\ninterpreter.register_component(\"depthwise_conv\", DepthwiseSeparableConvComponent)\n# Register mappings for common CV operations\ncv_transforms = {\n\"random_crop\": \"transforms.RandomCrop\",\n\"color_jitter\": \"transforms.ColorJitter\",\n\"random_erasing\": \"transforms.RandomErasing\"\n}\ninterpreter.register_mappings(\"data_transforms\", cv_transforms)\n</code></pre>"},{"location":"api/extensions/#publishing-and-sharing-extensions","title":"Publishing and Sharing Extensions","text":"<p>Extensions can be packaged and distributed as Python packages.</p>"},{"location":"api/extensions/#package-structure","title":"Package Structure","text":"<pre><code>nova-cv-extension/\n\u251c\u2500\u2500 setup.py\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 nova_cv/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 plugin.py\n\u2502   \u2514\u2500\u2500 components/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 resnet.py\n\u2502       \u251c\u2500\u2500 inception.py\n\u2502       \u2514\u2500\u2500 depthwise.py\n</code></pre>"},{"location":"api/extensions/#setuppy","title":"setup.py","text":"<pre><code>from setuptools import setup, find_packages\nsetup(\nname=\"nova-cv\",\nversion=\"0.1.0\",\npackages=find_packages(),\ninstall_requires=[\n\"nova-dl&gt;=0.1.0\",\n\"torch&gt;=1.8.0\",\n\"torchvision&gt;=0.9.0\"\n],\nentry_points={\n\"nova.plugins\": [\n\"cv=nova_cv.plugin:ComputerVisionPlugin\"\n]\n}\n)\n</code></pre>"},{"location":"api/extensions/#auto-discovery","title":"Auto-discovery","text":"<p>With the entry points configuration, Nova can automatically discover and load plugins:</p> <pre><code>from nova import NovaInterpreter\n# Create interpreter with auto-discovery\ninterpreter = NovaInterpreter(discover_plugins=True)\n</code></pre>"},{"location":"api/extensions/#next-steps","title":"Next Steps","text":"<ul> <li>Review the examples to see how to use extensions</li> <li>Check out the community page for guidelines on contributing extensions</li> <li>Explore the roadmap to see planned extensions and features</li> </ul>"},{"location":"api/interpreter/","title":"Nova Interpreter API","text":"<p>The Nova interpreter is the core component that translates Nova natural language code into executable PyTorch code. This document covers the public API of the interpreter, its classes, methods, and how to extend its functionality.</p>"},{"location":"api/interpreter/#novainterpreter-class","title":"NovaInterpreter Class","text":"<p>The main entry point for using Nova is the <code>NovaInterpreter</code> class.</p> <pre><code>from nova import NovaInterpreter\n# Create an interpreter instance\ninterpreter = NovaInterpreter()\n</code></pre>"},{"location":"api/interpreter/#key-methods","title":"Key Methods","text":""},{"location":"api/interpreter/#translate","title":"translate","text":"<pre><code>def translate(self, nova_code: str) -&gt; str:\n\"\"\"\n    Translate Nova code to PyTorch code.\n    Args:\n        nova_code: The Nova code to translate\n    Returns:\n        str: The corresponding PyTorch code\n    \"\"\"\n</code></pre> <p>This is the primary method for translating Nova code to PyTorch code. It takes a string containing Nova code and returns a string containing the equivalent PyTorch code.</p> <p>Example: <pre><code>nova_code = \"\"\"\ncreate processing pipeline simple_model:\n    add transformation stage fully_connected with 10 inputs and 5 outputs\n    apply relu activation\n    add transformation stage fully_connected with 5 inputs and 1 outputs\n\"\"\"\npytorch_code = interpreter.translate(nova_code)\nprint(pytorch_code)\n</code></pre></p>"},{"location":"api/interpreter/#explain_translation","title":"explain_translation","text":"<pre><code>def explain_translation(self, nova_code: str, pytorch_code: str) -&gt; str:\n\"\"\"\n    Generate an explanation of the translation from Nova to PyTorch.\n    Args:\n        nova_code: The original Nova code\n        pytorch_code: The translated PyTorch code\n    Returns:\n        str: An explanation of the translation\n    \"\"\"\n</code></pre> <p>This method generates a detailed explanation of how the Nova code was translated to PyTorch code. It's useful for understanding the translation process and learning PyTorch concepts.</p> <p>Example: <pre><code>explanation = interpreter.explain_translation(nova_code, pytorch_code)\nprint(explanation)\n</code></pre></p>"},{"location":"api/interpreter/#configuration-options","title":"Configuration Options","text":"<p>The <code>NovaInterpreter</code> class accepts several configuration options when instantiated:</p> <pre><code>interpreter = NovaInterpreter(\nverbose=False,           # Print additional information during translation\ninclude_comments=True,   # Include explanatory comments in the generated code\noptimize_code=False,     # Apply optimizations to the generated code\ncustom_mappings=None     # Custom mappings for extending the interpreter\n)\n</code></pre>"},{"location":"api/interpreter/#working-with-the-interpreter","title":"Working with the Interpreter","text":""},{"location":"api/interpreter/#basic-usage","title":"Basic Usage","text":"<pre><code>from nova import NovaInterpreter\n# Create an interpreter instance\ninterpreter = NovaInterpreter()\n# Define Nova code\nnova_code = \"\"\"\ncreate processing pipeline model:\n    add transformation stage fully_connected with 784 inputs and 128 outputs\n    apply relu activation\n    add transformation stage fully_connected with 128 inputs and 10 outputs\n\"\"\"\n# Translate to PyTorch\npytorch_code = interpreter.translate(nova_code)\n# Execute the generated code\nexec(pytorch_code)\n# Now the 'model' variable is available in the namespace\nprint(model)\n</code></pre>"},{"location":"api/interpreter/#interactive-usage","title":"Interactive Usage","text":"<pre><code>def interactive_session():\ninterpreter = NovaInterpreter()\nwhile True:\nprint(\"\\nEnter Nova code (or 'exit' to quit):\")\nlines = []\nwhile True:\nline = input()\nif line.strip() == \"\":\nbreak\nlines.append(line)\nnova_code = \"\\n\".join(lines)\nif nova_code.strip().lower() == \"exit\":\nbreak\ntry:\npytorch_code = interpreter.translate(nova_code)\nprint(\"\\nGenerated PyTorch code:\")\nprint(pytorch_code)\nprint(\"\\nExplanation:\")\nexplanation = interpreter.explain_translation(nova_code, pytorch_code)\nprint(explanation)\nexecute = input(\"\\nExecute the code? (y/n): \")\nif execute.lower() == 'y':\nexec(pytorch_code)\nprint(\"Code executed successfully.\")\nexcept Exception as e:\nprint(f\"Error: {e}\")\nif __name__ == \"__main__\":\ninteractive_session()\n</code></pre>"},{"location":"api/interpreter/#component-classes","title":"Component Classes","text":"<p>The interpreter consists of several component classes that handle different aspects of the translation process:</p>"},{"location":"api/interpreter/#novaparser","title":"NovaParser","text":"<p>Parses Nova code into an intermediate representation that can be processed by the translator.</p> <pre><code>from nova.parser import NovaParser\nparser = NovaParser()\ncomponents = parser.parse(nova_code)\n</code></pre>"},{"location":"api/interpreter/#novatranslator","title":"NovaTranslator","text":"<p>Translates the parsed components into PyTorch code.</p> <pre><code>from nova.translator import NovaTranslator\ntranslator = NovaTranslator()\npytorch_code = translator.translate(components)\n</code></pre>"},{"location":"api/interpreter/#novaexplainer","title":"NovaExplainer","text":"<p>Generates explanations of the translation process.</p> <pre><code>from nova.explainer import NovaExplainer\nexplainer = NovaExplainer()\nexplanation = explainer.explain(components, pytorch_code)\n</code></pre>"},{"location":"api/interpreter/#extending-the-interpreter","title":"Extending the Interpreter","text":"<p>The Nova interpreter is designed to be extensible, allowing you to add support for new operations, layers, or frameworks.</p>"},{"location":"api/interpreter/#custom-mappings","title":"Custom Mappings","text":"<p>You can provide custom mappings when creating an interpreter:</p> <pre><code>custom_mappings = {\n\"error_measures\": {\n\"custom_loss\": \"CustomLoss()\",\n},\n\"improvement_strategies\": {\n\"custom_optimizer\": \"CustomOptimizer\",\n},\n\"activations\": {\n\"custom_activation\": \"CustomActivation()\",\n}\n}\ninterpreter = NovaInterpreter(custom_mappings=custom_mappings)\n</code></pre>"},{"location":"api/interpreter/#creating-custom-components","title":"Creating Custom Components","text":"<p>For more complex extensions, you can create custom component classes:</p> <pre><code>from nova.components import LayerComponent\nclass CustomLayerComponent(LayerComponent):\ndef __init__(self, name, params):\nsuper().__init__(name, params)\ndef to_pytorch(self):\n# Generate PyTorch code for this component\nreturn f\"nn.CustomLayer({self.params['param']})\"\n# Register the component\nfrom nova.registry import register_component\nregister_component(\"custom_layer\", CustomLayerComponent)\n</code></pre>"},{"location":"api/interpreter/#plugin-system","title":"Plugin System","text":"<p>Nova supports a plugin system for more comprehensive extensions:</p> <pre><code>from nova.plugin import NovaPlugin\nclass MyNovaPlugin(NovaPlugin):\ndef __init__(self):\nsuper().__init__(\"my_plugin\")\ndef register(self, interpreter):\n# Register components, mappings, etc.\ninterpreter.register_mapping(\"error_measures\", \"my_loss\", \"MyLoss()\")\ndef unregister(self, interpreter):\n# Clean up when plugin is unregistered\ninterpreter.unregister_mapping(\"error_measures\", \"my_loss\")\n# Use the plugin\nfrom nova import NovaInterpreter\nfrom my_plugin import MyNovaPlugin\ninterpreter = NovaInterpreter()\nmy_plugin = MyNovaPlugin()\ninterpreter.register_plugin(my_plugin)\n</code></pre>"},{"location":"api/interpreter/#error-handling","title":"Error Handling","text":"<p>The Nova interpreter provides detailed error messages when it encounters issues in the Nova code:</p> <pre><code>try:\npytorch_code = interpreter.translate(nova_code)\nexcept nova.errors.SyntaxError as e:\nprint(f\"Syntax error: {e}\")\nexcept nova.errors.SemanticError as e:\nprint(f\"Semantic error: {e}\")\nexcept nova.errors.TranslationError as e:\nprint(f\"Translation error: {e}\")\nexcept Exception as e:\nprint(f\"Unknown error: {e}\")\n</code></pre>"},{"location":"api/interpreter/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>The interpreter is designed for interactive use and performance is generally not a bottleneck</li> <li>For large-scale batch processing, consider using a persistent interpreter instance</li> <li>The <code>optimize_code</code> option can improve the generated code's performance at the cost of readability</li> </ul>"},{"location":"api/interpreter/#next-steps","title":"Next Steps","text":"<p>Continue to the Extensions API to learn how to extend Nova with custom components and behavior.</p>"},{"location":"community/contributing/","title":"Contributing to Nova","text":"<p>Thank you for your interest in contributing to Nova! This document provides guidelines and instructions for contributing to the project. Nova is an open-source project, and we welcome contributions of all kinds, from bug reports to feature implementations.</p>"},{"location":"community/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>By participating in this project, you agree to abide by our Code of Conduct. We expect all contributors to be respectful, inclusive, and considerate of others.</p>"},{"location":"community/contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<p>There are many ways to contribute to Nova:</p>"},{"location":"community/contributing/#1-report-bugs","title":"1. Report Bugs","text":"<p>If you find a bug, please report it by creating an issue on GitHub. When reporting bugs, please include:</p> <ul> <li>A clear, descriptive title</li> <li>A detailed description of the issue</li> <li>Steps to reproduce the bug</li> <li>Expected behavior vs. actual behavior</li> <li>System information (OS, Python version, PyTorch version)</li> <li>Error messages or screenshots if applicable</li> </ul>"},{"location":"community/contributing/#2-suggest-features","title":"2. Suggest Features","text":"<p>If you have an idea for a new feature or enhancement, please create an issue on GitHub:</p> <ul> <li>Clearly describe the feature</li> <li>Explain why it would be valuable</li> <li>Provide examples of how it would be used</li> <li>If possible, outline how it might be implemented</li> </ul>"},{"location":"community/contributing/#3-improve-documentation","title":"3. Improve Documentation","text":"<p>Documentation improvements are always welcome:</p> <ul> <li>Fixing typos or grammar</li> <li>Clarifying confusing explanations</li> <li>Adding examples or tutorials</li> <li>Translating documentation to other languages</li> </ul>"},{"location":"community/contributing/#4-write-code","title":"4. Write Code","text":"<p>Contributing code is a great way to help improve Nova:</p> <ul> <li>Implement new features</li> <li>Fix bugs</li> <li>Optimize existing code</li> <li>Add test coverage</li> </ul>"},{"location":"community/contributing/#5-share-examples","title":"5. Share Examples","text":"<p>Creating examples showing how to use Nova for various tasks is very valuable:</p> <ul> <li>Create examples for different domains (computer vision, NLP, etc.)</li> <li>Show how to use Nova for specific tasks</li> <li>Compare Nova with other approaches</li> </ul>"},{"location":"community/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"community/contributing/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<ol> <li> <p>Fork and clone the repository:    <pre><code>git clone https://github.com/yourusername/nova.git\ncd nova\n</code></pre></p> </li> <li> <p>Create a virtual environment:    <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install development dependencies:    <pre><code>pip install -e \".[dev]\"\n</code></pre></p> </li> </ol>"},{"location":"community/contributing/#making-changes","title":"Making Changes","text":"<ol> <li> <p>Create a branch:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes: Implement your feature or bug fix.</p> </li> <li> <p>Follow the code style:</p> </li> <li>Use PEP 8 style guidelines</li> <li>Add type hints to new functions and methods</li> <li>Write docstrings in the Google style</li> <li> <p>Add appropriate comments for complex code</p> </li> <li> <p>Write tests:</p> </li> <li>Add tests for new features</li> <li>Fix or update tests for bug fixes</li> <li> <p>Ensure all tests pass</p> </li> <li> <p>Update documentation:</p> </li> <li>Update or add docstrings</li> <li>Update relevant documentation files</li> <li>Add examples if applicable</li> </ol>"},{"location":"community/contributing/#submitting-a-pull-request","title":"Submitting a Pull Request","text":"<ol> <li> <p>Commit your changes:    <pre><code>git add .\ngit commit -m \"Add a descriptive commit message\"\n</code></pre></p> </li> <li> <p>Push to your fork:    <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Create a pull request: Go to the Nova repository on GitHub and create a pull request from your fork.</p> </li> <li> <p>Describe your changes:</p> </li> <li>Provide a clear title and description</li> <li>Reference any related issues</li> <li>Explain the motivation for the changes</li> <li> <p>Describe how you tested the changes</p> </li> <li> <p>Address review feedback: If maintainers suggest changes, make them and push the updates to your branch.</p> </li> </ol>"},{"location":"community/contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"community/contributing/#code-style","title":"Code Style","text":"<p>We use Black and isort for code formatting. You can format your code with:</p> <pre><code>black .\nisort .\n</code></pre> <p>We use flake8 for linting. You can check your code with:</p> <pre><code>flake8\n</code></pre> <p>We use mypy for type checking. You can check your code with:</p> <pre><code>mypy nova\n</code></pre>"},{"location":"community/contributing/#testing","title":"Testing","text":"<p>We use pytest for testing. You can run the tests with:</p> <pre><code>pytest\n</code></pre> <p>For test coverage, we use pytest-cov. You can check coverage with:</p> <pre><code>pytest --cov=nova\n</code></pre>"},{"location":"community/contributing/#documentation","title":"Documentation","text":"<p>We use MkDocs with the Material theme for documentation. You can build and serve the documentation locally with:</p> <pre><code>cd website\npip install -r requirements.txt\nmkdocs serve\n</code></pre> <p>The documentation will be available at http://localhost:8000.</p>"},{"location":"community/contributing/#continuous-integration","title":"Continuous Integration","text":"<p>We use GitHub Actions for continuous integration. Pull requests will be automatically tested, and you will be notified of any failures.</p>"},{"location":"community/contributing/#project-structure","title":"Project Structure","text":"<p>Understanding the project structure will help you contribute effectively:</p> <pre><code>nova/\n\u251c\u2500\u2500 docs/            # Documentation\n\u251c\u2500\u2500 examples/        # Example usage\n\u251c\u2500\u2500 src/             # Source code\n\u2502   \u2514\u2500\u2500 nova/        # Main package\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 interpreter.py\n\u2502       \u251c\u2500\u2500 parser.py\n\u2502       \u251c\u2500\u2500 translator.py\n\u2502       \u251c\u2500\u2500 components/\n\u2502       \u251c\u2500\u2500 adapters/\n\u2502       \u2514\u2500\u2500 plugins/\n\u251c\u2500\u2500 tests/           # Test suite\n\u251c\u2500\u2500 website/         # Documentation website\n\u2514\u2500\u2500 setup.py         # Package configuration\n</code></pre>"},{"location":"community/contributing/#extending-nova","title":"Extending Nova","text":"<p>If you're interested in extending Nova with new components or features, see the Extensions API documentation.</p>"},{"location":"community/contributing/#adding-a-new-component","title":"Adding a New Component","text":"<ol> <li> <p>Create a new component class in the appropriate module:    <pre><code># src/nova/components/my_component.py\nfrom nova.components.base import Component\nclass MyComponent(Component):\ndef __init__(self, name, params):\nsuper().__init__(name, params)\ndef to_pytorch(self):\n# Generate PyTorch code\nreturn \"...\"\n</code></pre></p> </li> <li> <p>Register the component in the registry:    <pre><code># src/nova/components/__init__.py\nfrom nova.registry import register_component\nfrom .my_component import MyComponent\nregister_component(\"my_component\", MyComponent)\n</code></pre></p> </li> <li> <p>Add tests for the component:    <pre><code># tests/components/test_my_component.py\ndef test_my_component():\n# Test the component\n...\n</code></pre></p> </li> <li> <p>Add documentation for the component:    <pre><code>&lt;!-- docs/api/components.md --&gt;\n## MyComponent\nDescription of the component and how to use it.\n</code></pre></p> </li> </ol>"},{"location":"community/contributing/#adding-a-new-feature","title":"Adding a New Feature","text":"<ol> <li> <p>Implement the feature:    <pre><code># src/nova/feature.py\ndef my_feature():\n# Implement the feature\n...\n</code></pre></p> </li> <li> <p>Add tests for the feature:    <pre><code># tests/test_feature.py\ndef test_my_feature():\n# Test the feature\n...\n</code></pre></p> </li> <li> <p>Add documentation for the feature:    <pre><code>&lt;!-- docs/features.md --&gt;\n## My Feature\nDescription of the feature and how to use it.\n</code></pre></p> </li> </ol>"},{"location":"community/contributing/#community","title":"Community","text":"<p>Join our community to discuss Nova development, ask questions, and share ideas:</p> <ul> <li>GitHub Discussions: For questions, ideas, and general discussion</li> <li>GitHub Issues: For bug reports and feature requests</li> <li>Slack Channel: For real-time discussion (coming soon)</li> </ul>"},{"location":"community/contributing/#acknowledgements","title":"Acknowledgements","text":"<p>Your contributions are greatly appreciated. Contributors will be acknowledged in the project's README and release notes.</p>"},{"location":"community/contributing/#license","title":"License","text":"<p>By contributing to Nova, you agree that your contributions will be licensed under the project's MIT License.</p>"},{"location":"community/roadmap/","title":"Nova Roadmap","text":"<p>This document outlines the planned development roadmap for Nova. It provides a high-level view of our priorities, upcoming features, and long-term vision.</p>"},{"location":"community/roadmap/#current-status-version-010","title":"Current Status (Version 0.1.0)","text":"<p>The initial release of Nova includes:</p> <ul> <li>Basic Nova language syntax and semantics</li> <li>Translation of simple neural network models to PyTorch</li> <li>Support for common layers, activations, and training operations</li> <li>Basic documentation and examples</li> </ul>"},{"location":"community/roadmap/#short-term-goals-0-6-months","title":"Short-Term Goals (0-6 Months)","text":""},{"location":"community/roadmap/#version-020","title":"Version 0.2.0","text":"<ul> <li>Language Enhancements</li> <li>[ ] Support for more complex model architectures (residual networks, attention mechanisms)</li> <li>[ ] Advanced training options (learning rate scheduling, early stopping)</li> <li>[ ] Extended data loading and preprocessing operations</li> <li> <p>[ ] Custom loss functions and metrics</p> </li> <li> <p>Interpreter Improvements</p> </li> <li>[ ] Better error handling and reporting</li> <li>[ ] Performance optimizations for large models</li> <li>[ ] More detailed translation explanations</li> <li> <p>[ ] Support for generating executable notebooks</p> </li> <li> <p>Documentation and Examples</p> </li> <li>[ ] Comprehensive API reference</li> <li>[ ] More detailed tutorials</li> <li>[ ] Domain-specific examples (computer vision, NLP, time series)</li> <li>[ ] Interactive web playground</li> </ul>"},{"location":"community/roadmap/#version-030","title":"Version 0.3.0","text":"<ul> <li>Framework Support</li> <li>[ ] TensorFlow adapter for generating TensorFlow code</li> <li>[ ] JAX adapter for generating JAX code</li> <li> <p>[ ] ONNX export support</p> </li> <li> <p>Advanced Features</p> </li> <li>[ ] Automatic hyperparameter tuning</li> <li>[ ] Model visualization</li> <li>[ ] Distributed training support</li> <li> <p>[ ] Quantization and optimization</p> </li> <li> <p>Tool Integration</p> </li> <li>[ ] IDE plugins (VS Code, JupyterLab)</li> <li>[ ] CLI tools for batch processing</li> <li>[ ] Integration with experiment tracking systems (MLflow, Weights &amp; Biases)</li> </ul>"},{"location":"community/roadmap/#medium-term-goals-6-12-months","title":"Medium-Term Goals (6-12 Months)","text":""},{"location":"community/roadmap/#version-040-060","title":"Version 0.4.0 - 0.6.0","text":"<ul> <li>Language Expansion</li> <li>[ ] Domain-specific language extensions</li> <li>[ ] Higher-level abstractions for common patterns</li> <li>[ ] Support for custom architectures and components</li> <li> <p>[ ] Interactive model building</p> </li> <li> <p>Production Readiness</p> </li> <li>[ ] Deployment code generation (Docker, Kubernetes, TorchServe)</li> <li>[ ] Model serialization and versioning</li> <li>[ ] Performance benchmarking and optimization</li> <li> <p>[ ] Security and access control</p> </li> <li> <p>Ecosystem Integration</p> </li> <li>[ ] Integration with popular data science libraries</li> <li>[ ] Support for cloud platforms (AWS, GCP, Azure)</li> <li>[ ] Collaboration and sharing features</li> <li>[ ] Version control integration</li> </ul>"},{"location":"community/roadmap/#long-term-vision-1-years","title":"Long-Term Vision (1+ Years)","text":""},{"location":"community/roadmap/#version-100-and-beyond","title":"Version 1.0.0 and Beyond","text":"<ul> <li>Natural Language Interface</li> <li>[ ] Conversational model building</li> <li>[ ] Intelligent suggestions and autocomplete</li> <li>[ ] Explanation generation in natural language</li> <li> <p>[ ] Automatic code review and optimization</p> </li> <li> <p>Advanced Learning</p> </li> <li>[ ] Reinforcement learning support</li> <li>[ ] Transfer learning and fine-tuning</li> <li>[ ] Few-shot and zero-shot learning</li> <li> <p>[ ] Meta-learning and neural architecture search</p> </li> <li> <p>Research and Education</p> </li> <li>[ ] Interactive educational materials</li> <li>[ ] Research paper implementation assistance</li> <li>[ ] Benchmarking and reproducibility tools</li> <li>[ ] Collaboration features for teams</li> </ul>"},{"location":"community/roadmap/#feature-requests-and-prioritization","title":"Feature Requests and Prioritization","text":"<p>We welcome community input on feature prioritization. If you have a feature request or would like to see something moved up in priority, please:</p> <ol> <li>Check if it's already on the roadmap</li> <li>Submit a feature request on GitHub if it's not</li> <li>Vote on existing feature requests to help us understand community priorities</li> </ol>"},{"location":"community/roadmap/#contributing-to-the-roadmap","title":"Contributing to the Roadmap","text":"<p>If you're interested in contributing to items on the roadmap:</p> <ol> <li>Check the Contributing Guide for general contribution guidelines</li> <li>Look for issues labeled with \"roadmap\" on GitHub</li> <li>Comment on the issue to express your interest</li> <li>Submit a proposal or pull request with your implementation</li> </ol>"},{"location":"community/roadmap/#versioning-policy","title":"Versioning Policy","text":"<p>Nova follows Semantic Versioning:</p> <ul> <li>Major versions (1.0.0, 2.0.0): Introduce breaking changes</li> <li>Minor versions (0.1.0, 0.2.0): Add new features in a backward-compatible manner</li> <li>Patch versions (0.1.1, 0.1.2): Include backward-compatible bug fixes</li> </ul>"},{"location":"community/roadmap/#release-schedule","title":"Release Schedule","text":"<p>We aim to release:</p> <ul> <li>Patch releases: As needed for bug fixes</li> <li>Minor releases: Every 2-3 months</li> <li>Major releases: When significant breaking changes are necessary</li> </ul>"},{"location":"community/roadmap/#experimental-features","title":"Experimental Features","text":"<p>Some features may be released as experimental before being fully integrated:</p> <ul> <li>Experimental features will be clearly marked in the documentation</li> <li>They may change or be removed in future versions</li> <li>Feedback on experimental features is especially valuable</li> </ul>"},{"location":"community/roadmap/#deprecation-policy","title":"Deprecation Policy","text":"<p>When features need to be deprecated:</p> <ol> <li>They will be marked as deprecated in a minor release</li> <li>A migration path will be provided</li> <li>They will be removed in the next major release</li> <li>Deprecation notices will be included in release notes</li> </ol>"},{"location":"community/roadmap/#community-roadmap-meetings","title":"Community Roadmap Meetings","text":"<p>We hold regular community meetings to discuss roadmap priorities:</p> <ul> <li>Meetings are announced on GitHub and our community channels</li> <li>Anyone is welcome to attend and provide input</li> <li>Meeting notes and decisions are published for transparency</li> </ul> <p>Join us in shaping the future of Nova!</p>"},{"location":"examples/basic-models/","title":"Basic Models","text":"<p>This page demonstrates how to create and train simple neural network models using Nova. These examples are ideal for beginners getting started with Nova and deep learning.</p>"},{"location":"examples/basic-models/#simple-neural-network","title":"Simple Neural Network","text":"<p>Let's start with a basic fully-connected neural network for classifying MNIST digits.</p> Nova <pre><code># Load MNIST dataset\nload data collection mnist from torchvision.datasets with:\n    apply normalization with mean 0.1307 and deviation 0.3081\n    convert to feature grid\n\n# Prepare data streams\nprepare data stream train_stream from mnist.train with batch size 64 and shuffle enabled\nprepare data stream test_stream from mnist.test with batch size 1000\n\n# Create a neural network for digit classification\ncreate processing pipeline digit_classifier:\n    add transformation stage fully_connected with 784 inputs and 256 outputs\n    apply relu activation\n    add transformation stage fully_connected with 256 inputs and 128 outputs\n    apply relu activation\n    add transformation stage fully_connected with 128 inputs and 10 outputs\n\n# Train the model\ntrain digit_classifier on train_stream:\n    measure error using cross_entropy\n    improve using adam with learning rate 0.001\n    repeat for 5 learning cycles\n    print progress every 100 batches\n\n# Evaluate the model\nevaluate digit_classifier on test_stream:\n    measure accuracy\n    report results\n\n# Save the model\nsave digit_classifier to \"models/digit_classifier.pth\"\n</code></pre> PyTorch <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Load data\ntransform = transforms.Compose([\ntransforms.ToTensor(),\ntransforms.Normalize((0.1307,), (0.3081,))\n])\ntrain_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST('./data', train=False, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=1000)\n# Create model\nclass DigitClassifier(nn.Module):\ndef __init__(self):\nsuper(DigitClassifier, self).__init__()\nself.fc1 = nn.Linear(28*28, 256)\nself.relu1 = nn.ReLU()\nself.fc2 = nn.Linear(256, 128)\nself.relu2 = nn.ReLU()\nself.fc3 = nn.Linear(128, 10)\ndef forward(self, x):\nx = x.view(-1, 28*28)  # Flatten the input\nx = self.fc1(x)\nx = self.relu1(x)\nx = self.fc2(x)\nx = self.relu2(x)\nx = self.fc3(x)\nreturn x\nmodel = DigitClassifier().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n# Training function\ndef train(model, loader, criterion, optimizer, epoch):\nmodel.train()\nfor batch_idx, (data, target) in enumerate(loader):\ndata, target = data.to(device), target.to(device)\noptimizer.zero_grad()\noutput = model(data)\nloss = criterion(output, target)\nloss.backward()\noptimizer.step()\nif batch_idx % 100 == 0:\nprint(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loader.dataset)}'\nf' ({100. * batch_idx / len(loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n# Evaluation function\ndef test(model, loader, criterion):\nmodel.eval()\ntest_loss = 0\ncorrect = 0\nwith torch.no_grad():\nfor data, target in loader:\ndata, target = data.to(device), target.to(device)\noutput = model(data)\ntest_loss += criterion(output, target).item()\npred = output.argmax(dim=1, keepdim=True)\ncorrect += pred.eq(target.view_as(pred)).sum().item()\ntest_loss /= len(loader.dataset)\naccuracy = 100. * correct / len(loader.dataset)\nprint(f'\\nTest set: Average loss: {test_loss:.4f}, '\nf'Accuracy: {correct}/{len(loader.dataset)} ({accuracy:.0f}%)\\n')\nreturn accuracy\n# Train model\nfor epoch in range(1, 6):\ntrain(model, train_loader, criterion, optimizer, epoch)\ntest(model, test_loader, criterion)\n# Save model\ntorch.save(model.state_dict(), \"models/digit_classifier.pth\")\n</code></pre>"},{"location":"examples/basic-models/#key-components-explained","title":"Key Components Explained","text":"<ol> <li>Data Loading: Load the MNIST dataset with normalization</li> <li>Data Preparation: Create data loaders for training and testing</li> <li>Model Definition: Create a neural network with three fully-connected layers</li> <li>Training Setup: Define the loss function and optimizer</li> <li>Training Loop: Train the model for 5 epochs</li> <li>Evaluation: Evaluate the model on the test set</li> <li>Model Saving: Save the trained model</li> </ol>"},{"location":"examples/basic-models/#logistic-regression","title":"Logistic Regression","text":"<p>Logistic regression is one of the simplest models for binary classification. Here's how to implement it in Nova:</p> Nova <pre><code># Create binary classification dataset\nload data collection binary_dataset from sklearn.datasets.make_classification with:\n    100 samples\n    2 features\n    2 classes\n\n# Split dataset\nsplit binary_dataset into 70% training and 30% testing\n\n# Create a logistic regression model\ncreate processing pipeline logistic_model:\n    add transformation stage fully_connected with 2 inputs and 1 outputs\n    apply sigmoid activation\n\n# Train the model\ntrain logistic_model on train_stream:\n    measure error using binary_cross_entropy\n    improve using gradient_descent with learning rate 0.05\n    repeat for 100 learning cycles\n\n# Evaluate the model\nevaluate logistic_model on test_stream:\n    measure accuracy\n    report results\n</code></pre> PyTorch <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader\n# Create dataset\nX, y = make_classification(n_samples=100, n_features=2, n_classes=2, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n# Convert to PyTorch tensors\nX_train = torch.FloatTensor(X_train)\nX_test = torch.FloatTensor(X_test)\ny_train = torch.FloatTensor(y_train).view(-1, 1)\ny_test = torch.FloatTensor(y_test).view(-1, 1)\n# Create data loaders\ntrain_dataset = TensorDataset(X_train, y_train)\ntest_dataset = TensorDataset(X_test, y_test)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n# Create model\nclass LogisticModel(nn.Module):\ndef __init__(self):\nsuper(LogisticModel, self).__init__()\nself.linear = nn.Linear(2, 1)\nself.sigmoid = nn.Sigmoid()\ndef forward(self, x):\nx = self.linear(x)\nx = self.sigmoid(x)\nreturn x\nmodel = LogisticModel()\ncriterion = nn.BCELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.05)\n# Training loop\nfor epoch in range(100):\nfor inputs, targets in train_loader:\n# Zero the parameter gradients\noptimizer.zero_grad()\n# Forward pass\noutputs = model(inputs)\nloss = criterion(outputs, targets)\n# Backward pass and optimize\nloss.backward()\noptimizer.step()\nif (epoch + 1) % 10 == 0:\nprint(f'Epoch {epoch+1}/100, Loss: {loss.item():.4f}')\n# Evaluation\nmodel.eval()\nwith torch.no_grad():\ncorrect = 0\ntotal = 0\nfor inputs, targets in test_loader:\noutputs = model(inputs)\npredicted = (outputs &gt; 0.5).float()\ntotal += targets.size(0)\ncorrect += (predicted == targets).sum().item()\naccuracy = 100 * correct / total\nprint(f'Accuracy: {accuracy:.2f}%')\n</code></pre>"},{"location":"examples/basic-models/#key-components-explained_1","title":"Key Components Explained","text":"<ol> <li>Dataset Creation: Generate a synthetic binary classification dataset</li> <li>Data Preparation: Split the dataset and create data loaders</li> <li>Model Definition: Create a logistic regression model (a single neuron with sigmoid activation)</li> <li>Training Setup: Define the binary cross-entropy loss and gradient descent optimizer</li> <li>Training Loop: Train the model for 100 epochs</li> <li>Evaluation: Evaluate the model accuracy on the test set</li> </ol>"},{"location":"examples/basic-models/#multi-layer-perceptron-mlp","title":"Multi-Layer Perceptron (MLP)","text":"<p>A Multi-Layer Perceptron (MLP) is a fully-connected neural network with multiple hidden layers.</p> Nova <pre><code># Load iris dataset\nload data collection iris from sklearn.datasets.load_iris\nsplit iris into 80% training and 20% testing\n\n# Prepare data streams\nprepare data stream train_stream from iris.train with batch size 16 and shuffle enabled\nprepare data stream test_stream from iris.test with batch size 32\n\n# Create a multi-layer perceptron for classification\ncreate processing pipeline iris_classifier:\n    add transformation stage fully_connected with 4 inputs and 64 outputs\n    apply relu activation\n    add dropout with rate 0.2\n    add transformation stage fully_connected with 64 inputs and 32 outputs\n    apply relu activation\n    add dropout with rate 0.2\n    add transformation stage fully_connected with 32 inputs and 3 outputs\n    apply softmax activation\n\n# Train the model\ntrain iris_classifier on train_stream:\n    measure error using cross_entropy\n    improve using adam with learning rate 0.001 and weight decay 0.0001\n    repeat for 100 learning cycles\n\n# Evaluate the model\nevaluate iris_classifier on test_stream:\n    measure accuracy, precision, recall, and f1 score\n    report results\n</code></pre> PyTorch <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom torch.utils.data import TensorDataset, DataLoader\n# Load dataset\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Convert to PyTorch tensors\nX_train = torch.FloatTensor(X_train)\nX_test = torch.FloatTensor(X_test)\ny_train = torch.LongTensor(y_train)\ny_test = torch.LongTensor(y_test)\n# Create data loaders\ntrain_dataset = TensorDataset(X_train, y_train)\ntest_dataset = TensorDataset(X_test, y_test)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n# Create model\nclass IrisClassifier(nn.Module):\ndef __init__(self):\nsuper(IrisClassifier, self).__init__()\nself.fc1 = nn.Linear(4, 64)\nself.relu1 = nn.ReLU()\nself.dropout1 = nn.Dropout(0.2)\nself.fc2 = nn.Linear(64, 32)\nself.relu2 = nn.ReLU()\nself.dropout2 = nn.Dropout(0.2)\nself.fc3 = nn.Linear(32, 3)\nself.softmax = nn.Softmax(dim=1)\ndef forward(self, x):\nx = self.fc1(x)\nx = self.relu1(x)\nx = self.dropout1(x)\nx = self.fc2(x)\nx = self.relu2(x)\nx = self.dropout2(x)\nx = self.fc3(x)\nx = self.softmax(x)\nreturn x\nmodel = IrisClassifier()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n# Training loop\nfor epoch in range(100):\nmodel.train()\nfor inputs, targets in train_loader:\noptimizer.zero_grad()\noutputs = model(inputs)\nloss = criterion(outputs, targets)\nloss.backward()\noptimizer.step()\nif (epoch + 1) % 10 == 0:\nprint(f'Epoch {epoch+1}/100, Loss: {loss.item():.4f}')\n# Evaluation\nmodel.eval()\nwith torch.no_grad():\nall_preds = []\nall_targets = []\nfor inputs, targets in test_loader:\noutputs = model(inputs)\n_, predicted = torch.max(outputs, 1)\nall_preds.extend(predicted.cpu().numpy())\nall_targets.extend(targets.cpu().numpy())\n# Calculate metrics\naccuracy = 100 * np.mean(np.array(all_preds) == np.array(all_targets))\nprecision = precision_score(all_targets, all_preds, average='macro')\nrecall = recall_score(all_targets, all_preds, average='macro')\nf1 = f1_score(all_targets, all_preds, average='macro')\nprint(f'Accuracy: {accuracy:.2f}%')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')\n</code></pre>"},{"location":"examples/basic-models/#key-components-explained_2","title":"Key Components Explained","text":"<ol> <li>Dataset Loading: Load the Iris dataset for multi-class classification</li> <li>Data Preparation: Split the dataset and create data loaders</li> <li>Model Definition: Create an MLP with two hidden layers, ReLU activations, and dropout regularization</li> <li>Training Setup: Define the cross-entropy loss and Adam optimizer with weight decay</li> <li>Training Loop: Train the model for 100 epochs</li> <li>Evaluation: Evaluate the model using multiple metrics (accuracy, precision, recall, F1 score)</li> </ol>"},{"location":"examples/basic-models/#next-steps","title":"Next Steps","text":"<p>Now that you've learned about basic models, you can explore more complex architectures:</p> <ul> <li>Computer Vision: Creating CNNs for image tasks</li> <li>NLP: Building RNNs and transformers for text</li> <li>Advanced Topics: Exploring advanced techniques like transfer learning and generative models</li> </ul>"},{"location":"examples/computer-vision/","title":"Computer Vision Examples","text":"<p>This page demonstrates how to use Nova for computer vision tasks using convolutional neural networks (CNNs) and related architectures.</p>"},{"location":"examples/computer-vision/#basic-cnn-for-mnist","title":"Basic CNN for MNIST","text":"<p>Let's start with a simple CNN for digit classification on the MNIST dataset.</p> Nova <pre><code># Load MNIST dataset\nload data collection mnist from torchvision.datasets with:\n    apply normalization with mean 0.1307 and deviation 0.3081\n    convert to feature grid\n\n# Prepare data streams\nprepare data stream train_stream from mnist.train with batch size 64 and shuffle enabled\nprepare data stream test_stream from mnist.test with batch size 1000\n\n# Create a CNN for digit classification\ncreate image processing pipeline mnist_cnn:\n    # First convolutional block\n    add feature detector with 1 input channels, 32 output channels and 3x3 filter size\n    apply relu activation\n    add feature detector with 32 input channels, 32 output channels and 3x3 filter size\n    apply relu activation\n    add downsampling using max method with size 2x2\n\n    # Second convolutional block\n    add feature detector with 32 input channels, 64 output channels and 3x3 filter size\n    apply relu activation\n    add feature detector with 64 input channels, 64 output channels and 3x3 filter size\n    apply relu activation\n    add downsampling using max method with size 2x2\n\n    # Fully connected layers\n    flatten features\n    add transformation stage fully_connected with 1600 inputs and 128 outputs\n    apply relu activation\n    add dropout with rate 0.5\n    add transformation stage fully_connected with 128 inputs and 10 outputs\n\n# Train the model\ntrain mnist_cnn on train_stream:\n    measure error using cross_entropy\n    improve using adam with learning rate 0.001\n    repeat for 10 learning cycles\n    print progress every 100 batches\n\n# Evaluate the model\nevaluate mnist_cnn on test_stream:\n    measure accuracy\n    report results\n</code></pre> PyTorch <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Load data\ntransform = transforms.Compose([\ntransforms.ToTensor(),\ntransforms.Normalize((0.1307,), (0.3081,))\n])\ntrain_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST('./data', train=False, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=1000)\n# Create model\nclass MnistCNN(nn.Module):\ndef __init__(self):\nsuper(MnistCNN, self).__init__()\n# First convolutional block\nself.conv1_1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\nself.relu1_1 = nn.ReLU()\nself.conv1_2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\nself.relu1_2 = nn.ReLU()\nself.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n# Second convolutional block\nself.conv2_1 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\nself.relu2_1 = nn.ReLU()\nself.conv2_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\nself.relu2_2 = nn.ReLU()\nself.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n# Fully connected layers\nself.fc1 = nn.Linear(1600, 128)  # 7x7x64 = 3136\nself.relu3 = nn.ReLU()\nself.dropout = nn.Dropout(0.5)\nself.fc2 = nn.Linear(128, 10)\ndef forward(self, x):\n# First convolutional block\nx = self.relu1_1(self.conv1_1(x))\nx = self.relu1_2(self.conv1_2(x))\nx = self.pool1(x)\n# Second convolutional block\nx = self.relu2_1(self.conv2_1(x))\nx = self.relu2_2(self.conv2_2(x))\nx = self.pool2(x)\n# Flatten and fully connected layers\nx = x.view(x.size(0), -1)  # Flatten\nx = self.relu3(self.fc1(x))\nx = self.dropout(x)\nx = self.fc2(x)\nreturn x\nmodel = MnistCNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n# Training function\ndef train(model, loader, criterion, optimizer, epoch):\nmodel.train()\nfor batch_idx, (data, target) in enumerate(loader):\ndata, target = data.to(device), target.to(device)\noptimizer.zero_grad()\noutput = model(data)\nloss = criterion(output, target)\nloss.backward()\noptimizer.step()\nif batch_idx % 100 == 0:\nprint(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loader.dataset)}'\nf' ({100. * batch_idx / len(loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n# Evaluation function\ndef test(model, loader):\nmodel.eval()\ntest_loss = 0\ncorrect = 0\nwith torch.no_grad():\nfor data, target in loader:\ndata, target = data.to(device), target.to(device)\noutput = model(data)\npred = output.argmax(dim=1, keepdim=True)\ncorrect += pred.eq(target.view_as(pred)).sum().item()\naccuracy = 100. * correct / len(loader.dataset)\nprint(f'Test Accuracy: {accuracy:.2f}%')\nreturn accuracy\n# Train and evaluate model\nfor epoch in range(1, 11):\ntrain(model, train_loader, criterion, optimizer, epoch)\naccuracy = test(model, test_loader)\n</code></pre>"},{"location":"examples/computer-vision/#key-components-explained","title":"Key Components Explained","text":"<ol> <li>Convolutional Layers: Used to extract spatial features from images</li> <li>Pooling Layers: Reduce the spatial dimensions while preserving important features</li> <li>Fully Connected Layers: Connect all extracted features for final classification</li> <li>Dropout: Prevents overfitting by randomly deactivating neurons during training</li> </ol>"},{"location":"examples/computer-vision/#cnn-for-cifar-10","title":"CNN for CIFAR-10","text":"<p>Let's build a more complex CNN for the CIFAR-10 dataset, which consists of 10 classes of color images.</p> Nova <pre><code># Load CIFAR-10 dataset\nload data collection cifar10 from torchvision.datasets with:\n    apply image transformations:\n        resize to 32x32\n        random horizontal flip with probability 0.5\n        random crop to 32x32 with padding 4\n        convert to tensor\n        normalize with mean [0.4914, 0.4822, 0.4465] and deviation [0.2470, 0.2435, 0.2616]\n\n# Prepare data streams\nprepare data stream train_stream from cifar10.train with batch size 128 and shuffle enabled\nprepare data stream test_stream from cifar10.test with batch size 1000\n\n# Create a CNN for CIFAR-10 classification\ncreate image processing pipeline cifar_cnn:\n    # First convolutional block\n    add feature detector with 3 input channels, 64 output channels and 3x3 filter size and padding same\n    apply batch normalization with 64 features\n    apply relu activation\n    add feature detector with 64 input channels, 64 output channels and 3x3 filter size and padding same\n    apply batch normalization with 64 features\n    apply relu activation\n    add downsampling using max method with size 2x2\n    add dropout with rate 0.25\n\n    # Second convolutional block\n    add feature detector with 64 input channels, 128 output channels and 3x3 filter size and padding same\n    apply batch normalization with 128 features\n    apply relu activation\n    add feature detector with 128 input channels, 128 output channels and 3x3 filter size and padding same\n    apply batch normalization with 128 features\n    apply relu activation\n    add downsampling using max method with size 2x2\n    add dropout with rate 0.25\n\n    # Third convolutional block\n    add feature detector with 128 input channels, 256 output channels and 3x3 filter size and padding same\n    apply batch normalization with 256 features\n    apply relu activation\n    add feature detector with 256 input channels, 256 output channels and 3x3 filter size and padding same\n    apply batch normalization with 256 features\n    apply relu activation\n    add downsampling using max method with size 2x2\n    add dropout with rate 0.25\n\n    # Fully connected layers\n    flatten features\n    add transformation stage fully_connected with 4096 inputs and 512 outputs\n    apply batch normalization with 512 features\n    apply relu activation\n    add dropout with rate 0.5\n    add transformation stage fully_connected with 512 inputs and 10 outputs\n\n# Train the model\ntrain cifar_cnn on train_stream:\n    measure error using cross_entropy\n    improve using adam with learning rate 0.001 and weight decay 0.0001\n    reduce learning rate by factor 0.1 when plateau in validation loss for 5 cycles\n    repeat for 50 learning cycles\n    stop early if no improvement for 10 cycles\n    print progress every 100 batches\n\n# Evaluate the model\nevaluate cifar_cnn on test_stream:\n    measure accuracy\n    report results\n</code></pre> PyTorch <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Data transformations\ntransform_train = transforms.Compose([\ntransforms.Resize((32, 32)),\ntransforms.RandomHorizontalFlip(p=0.5),\ntransforms.RandomCrop(32, padding=4),\ntransforms.ToTensor(),\ntransforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n])\ntransform_test = transforms.Compose([\ntransforms.Resize((32, 32)),\ntransforms.ToTensor(),\ntransforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n])\n# Load datasets\ntrain_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\ntest_dataset = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=1000)\n# Define the model\nclass CifarCNN(nn.Module):\ndef __init__(self):\nsuper(CifarCNN, self).__init__()\n# First convolutional block\nself.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\nself.bn1_1 = nn.BatchNorm2d(64)\nself.relu1_1 = nn.ReLU()\nself.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\nself.bn1_2 = nn.BatchNorm2d(64)\nself.relu1_2 = nn.ReLU()\nself.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\nself.dropout1 = nn.Dropout(0.25)\n# Second convolutional block\nself.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\nself.bn2_1 = nn.BatchNorm2d(128)\nself.relu2_1 = nn.ReLU()\nself.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\nself.bn2_2 = nn.BatchNorm2d(128)\nself.relu2_2 = nn.ReLU()\nself.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\nself.dropout2 = nn.Dropout(0.25)\n# Third convolutional block\nself.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\nself.bn3_1 = nn.BatchNorm2d(256)\nself.relu3_1 = nn.ReLU()\nself.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\nself.bn3_2 = nn.BatchNorm2d(256)\nself.relu3_2 = nn.ReLU()\nself.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\nself.dropout3 = nn.Dropout(0.25)\n# Fully connected layers\nself.fc1 = nn.Linear(4096, 512)\nself.bn4 = nn.BatchNorm1d(512)\nself.relu4 = nn.ReLU()\nself.dropout4 = nn.Dropout(0.5)\nself.fc2 = nn.Linear(512, 10)\ndef forward(self, x):\n# First block\nx = self.relu1_1(self.bn1_1(self.conv1_1(x)))\nx = self.relu1_2(self.bn1_2(self.conv1_2(x)))\nx = self.pool1(x)\nx = self.dropout1(x)\n# Second block\nx = self.relu2_1(self.bn2_1(self.conv2_1(x)))\nx = self.relu2_2(self.bn2_2(self.conv2_2(x)))\nx = self.pool2(x)\nx = self.dropout2(x)\n# Third block\nx = self.relu3_1(self.bn3_1(self.conv3_1(x)))\nx = self.relu3_2(self.bn3_2(self.conv3_2(x)))\nx = self.pool3(x)\nx = self.dropout3(x)\n# Flatten and fully connected layers\nx = x.view(x.size(0), -1)  # Flatten\nx = self.relu4(self.bn4(self.fc1(x)))\nx = self.dropout4(x)\nx = self.fc2(x)\nreturn x\nmodel = CifarCNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\nscheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5, verbose=True)\n# Training function\ndef train(model, loader, criterion, optimizer):\nmodel.train()\nrunning_loss = 0.0\ncorrect = 0\ntotal = 0\nfor batch_idx, (data, target) in enumerate(loader):\ndata, target = data.to(device), target.to(device)\noptimizer.zero_grad()\noutput = model(data)\nloss = criterion(output, target)\nloss.backward()\noptimizer.step()\nrunning_loss += loss.item()\n_, predicted = output.max(1)\ntotal += target.size(0)\ncorrect += predicted.eq(target).sum().item()\nif batch_idx % 100 == 0:\nprint(f'Batch {batch_idx}: Loss: {running_loss/(batch_idx+1):.4f} | '\nf'Acc: {100.*correct/total:.2f}%')\nreturn running_loss / len(loader)\n# Evaluation function\ndef test(model, loader):\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\nfor data, target in loader:\ndata, target = data.to(device), target.to(device)\noutput = model(data)\n_, predicted = output.max(1)\ntotal += target.size(0)\ncorrect += predicted.eq(target).sum().item()\naccuracy = 100. * correct / total\nprint(f'Test Accuracy: {accuracy:.2f}%')\nreturn accuracy\n# Training loop with early stopping\nbest_loss = float('inf')\npatience_counter = 0\nfor epoch in range(50):\nprint(f'Epoch {epoch+1}/50')\ntrain_loss = train(model, train_loader, criterion, optimizer)\naccuracy = test(model, test_loader)\n# Learning rate scheduling\nscheduler.step(train_loss)\n# Early stopping\nif train_loss &lt; best_loss:\nbest_loss = train_loss\npatience_counter = 0\n# Save best model\ntorch.save(model.state_dict(), 'cifar_cnn_best.pth')\nelse:\npatience_counter += 1\nif patience_counter &gt;= 10:\nprint(f'Early stopping triggered after {epoch+1} epochs')\nbreak\n</code></pre>"},{"location":"examples/computer-vision/#advanced-techniques-explained","title":"Advanced Techniques Explained","text":"<ol> <li>Data Augmentation: Random horizontal flips and crops increase the effective training set size</li> <li>Batch Normalization: Stabilizes and accelerates training by normalizing layer inputs</li> <li>Multiple Convolutional Blocks: Deeper networks can learn more complex features</li> <li>Learning Rate Scheduling: Reduces learning rate when improvement plateaus</li> <li>Early Stopping: Prevents overfitting by stopping training when validation loss stops improving</li> </ol>"},{"location":"examples/computer-vision/#image-segmentation","title":"Image Segmentation","text":"<p>Image segmentation is the task of assigning a class label to each pixel in an image. Let's implement a simple U-Net model for segmentation.</p> Nova <pre><code># Load segmentation dataset\nload data collection segmentation_dataset from custom_loader with:\n    apply image transformations:\n        resize to 128x128\n        convert to tensor\n        normalize with mean [0.5, 0.5, 0.5] and deviation [0.5, 0.5, 0.5]\n\n# Create U-Net architecture for segmentation\ncreate image processing pipeline unet:\n    # Encoder path (downsampling)\n    # Block 1\n    add feature detector with 3 input channels, 64 output channels and 3x3 filter size and padding same\n    apply relu activation\n    add feature detector with 64 input channels, 64 output channels and 3x3 filter size and padding same\n    apply relu activation\n    save features as skip1\n    add downsampling using max method with size 2x2\n\n    # Block 2\n    add feature detector with 64 input channels, 128 output channels and 3x3 filter size and padding same\n    apply relu activation\n    add feature detector with 128 input channels, 128 output channels and 3x3 filter size and padding same\n    apply relu activation\n    save features as skip2\n    add downsampling using max method with size 2x2\n\n    # Block 3\n    add feature detector with 128 input channels, 256 output channels and 3x3 filter size and padding same\n    apply relu activation\n    add feature detector with 256 input channels, 256 output channels and 3x3 filter size and padding same\n    apply relu activation\n    save features as skip3\n    add downsampling using max method with size 2x2\n\n    # Bridge\n    add feature detector with 256 input channels, 512 output channels and 3x3 filter size and padding same\n    apply relu activation\n    add feature detector with 512 input channels, 512 output channels and 3x3 filter size and padding same\n    apply relu activation\n\n    # Decoder path (upsampling)\n    # Block 3\n    add upsampling with scale 2x2\n    add feature detector with 512 input channels, 256 output channels and 3x3 filter size and padding same\n    apply relu activation\n    combine with skip3\n    add feature detector with 512 input channels, 256 output channels and 3x3 filter size and padding same\n    apply relu activation\n\n    # Block 2\n    add upsampling with scale 2x2\n    add feature detector with 256 input channels, 128 output channels and 3x3 filter size and padding same\n    apply relu activation\n    combine with skip2\n    add feature detector with 256 input channels, 128 output channels and 3x3 filter size and padding same\n    apply relu activation\n\n    # Block 1\n    add upsampling with scale 2x2\n    add feature detector with 128 input channels, 64 output channels and 3x3 filter size and padding same\n    apply relu activation\n    combine with skip1\n    add feature detector with 128 input channels, 64 output channels and 3x3 filter size and padding same\n    apply relu activation\n\n    # Output layer\n    add feature detector with 64 input channels, 1 output channels and 1x1 filter size\n    apply sigmoid activation\n\n# Train the segmentation model\ntrain unet on train_stream:\n    measure error using dice_loss\n    improve using adam with learning rate 0.0001\n    repeat for 50 learning cycles\n    print progress every 10 batches\n</code></pre> PyTorch <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n# Define U-Net model\nclass UNet(nn.Module):\ndef __init__(self):\nsuper(UNet, self).__init__()\n# Encoder (downsampling)\n# Block 1\nself.enc1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\nself.enc1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\nself.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n# Block 2\nself.enc2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\nself.enc2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\nself.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n# Block 3\nself.enc3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\nself.enc3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\nself.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n# Bridge\nself.bridge1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\nself.bridge2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n# Decoder (upsampling)\n# Block 3\nself.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\nself.dec3_1 = nn.Conv2d(512, 256, kernel_size=3, padding=1)  # 512 = 256 + 256 (skip connection)\nself.dec3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n# Block 2\nself.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\nself.dec2_1 = nn.Conv2d(256, 128, kernel_size=3, padding=1)  # 256 = 128 + 128 (skip connection)\nself.dec2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n# Block 1\nself.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\nself.dec1_1 = nn.Conv2d(128, 64, kernel_size=3, padding=1)   # 128 = 64 + 64 (skip connection)\nself.dec1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n# Output layer\nself.out = nn.Conv2d(64, 1, kernel_size=1)\ndef forward(self, x):\n# Encoder\n# Block 1\nenc1 = F.relu(self.enc1_1(x))\nenc1 = F.relu(self.enc1_2(enc1))\nenc1_pool = self.pool1(enc1)\n# Block 2\nenc2 = F.relu(self.enc2_1(enc1_pool))\nenc2 = F.relu(self.enc2_2(enc2))\nenc2_pool = self.pool2(enc2)\n# Block 3\nenc3 = F.relu(self.enc3_1(enc2_pool))\nenc3 = F.relu(self.enc3_2(enc3))\nenc3_pool = self.pool3(enc3)\n# Bridge\nbridge = F.relu(self.bridge1(enc3_pool))\nbridge = F.relu(self.bridge2(bridge))\n# Decoder\n# Block 3\nup3 = self.upconv3(bridge)\nup3 = torch.cat([up3, enc3], dim=1)  # Skip connection\ndec3 = F.relu(self.dec3_1(up3))\ndec3 = F.relu(self.dec3_2(dec3))\n# Block 2\nup2 = self.upconv2(dec3)\nup2 = torch.cat([up2, enc2], dim=1)  # Skip connection\ndec2 = F.relu(self.dec2_1(up2))\ndec2 = F.relu(self.dec2_2(dec2))\n# Block 1\nup1 = self.upconv1(dec2)\nup1 = torch.cat([up1, enc1], dim=1)  # Skip connection\ndec1 = F.relu(self.dec1_1(up1))\ndec1 = F.relu(self.dec1_2(dec1))\n# Output\nout = torch.sigmoid(self.out(dec1))\nreturn out\n# Define Dice loss for segmentation\nclass DiceLoss(nn.Module):\ndef __init__(self, smooth=1.0):\nsuper(DiceLoss, self).__init__()\nself.smooth = smooth\ndef forward(self, pred, target):\npred = pred.view(-1)\ntarget = target.view(-1)\nintersection = (pred * target).sum()\ndice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\nreturn 1 - dice\n# Create model, loss, and optimizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = UNet().to(device)\ncriterion = DiceLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n# Training function\ndef train(model, loader, criterion, optimizer, epoch):\nmodel.train()\nrunning_loss = 0.0\nfor batch_idx, (data, target) in enumerate(loader):\ndata, target = data.to(device), target.to(device)\noptimizer.zero_grad()\noutput = model(data)\nloss = criterion(output, target)\nloss.backward()\noptimizer.step()\nrunning_loss += loss.item()\nif batch_idx % 10 == 0:\nprint(f'Epoch {epoch+1}, Batch {batch_idx+1}: Loss: {loss.item():.4f}')\navg_loss = running_loss / len(loader)\nprint(f'Epoch {epoch+1} completed. Average loss: {avg_loss:.4f}')\nreturn avg_loss\n# Training loop\nfor epoch in range(50):\ntrain(model, train_loader, criterion, optimizer, epoch)\n</code></pre>"},{"location":"examples/computer-vision/#u-net-architecture-explained","title":"U-Net Architecture Explained","text":"<ol> <li>Encoder Path (Downsampling):</li> <li>Captures context and reduces spatial dimensions</li> <li> <p>Uses convolutional blocks and max pooling</p> </li> <li> <p>Bridge:</p> </li> <li>Connects the encoder and decoder paths</li> <li> <p>Has the lowest resolution but the highest feature depth</p> </li> <li> <p>Decoder Path (Upsampling):</p> </li> <li>Recovers spatial information for precise localization</li> <li> <p>Uses transposed convolutions (upsampling)</p> </li> <li> <p>Skip Connections:</p> </li> <li>Connect corresponding layers in the encoder and decoder</li> <li> <p>Help preserve spatial details that would otherwise be lost during downsampling</p> </li> <li> <p>Output Layer:</p> </li> <li>Maps features to the desired number of classes</li> <li>Uses sigmoid activation for binary segmentation</li> </ol>"},{"location":"examples/computer-vision/#object-detection","title":"Object Detection","text":"<p>Here's an example of how to implement a simple object detection model using Nova:</p> Nova <pre><code># Create a simple single-shot object detector (SSD-like)\ncreate image processing pipeline object_detector:\n    # Base network (feature extractor)\n    add feature detector with 3 input channels, 64 output channels and 3x3 filter size\n    apply relu activation\n    add downsampling using max method with size 2x2\n\n    add feature detector with 64 input channels, 128 output channels and 3x3 filter size\n    apply relu activation\n    add downsampling using max method with size 2x2\n\n    add feature detector with 128 input channels, 256 output channels and 3x3 filter size\n    apply relu activation\n    add downsampling using max method with size 2x2\n\n    add feature detector with 256 input channels, 512 output channels and 3x3 filter size\n    apply relu activation\n\n    # Detection heads\n    # Classification head\n    add parallel branch classification_head:\n        add feature detector with 512 input channels, 256 output channels and 3x3 filter size\n        apply relu activation\n        add feature detector with 256 input channels, num_classes * num_anchors output channels and 1x1 filter size\n        reshape to [batch_size, num_anchors, num_classes]\n        apply softmax activation on class dimension\n\n    # Bounding box regression head\n    add parallel branch bbox_head:\n        add feature detector with 512 input channels, 256 output channels and 3x3 filter size\n        apply relu activation\n        add feature detector with 256 input channels, 4 * num_anchors output channels and 1x1 filter size\n        reshape to [batch_size, num_anchors, 4]\n\n# Train with specialized object detection loss\ntrain object_detector on coco_dataloader:\n    measure error using detection_loss:\n        classification component using focal_loss with alpha 0.25 and gamma 2.0\n        bounding box component using smooth_l1_loss\n    improve using adam with learning rate 0.0001\n    repeat for 100 learning cycles\n</code></pre> PyTorch <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass FocalLoss(nn.Module):\ndef __init__(self, alpha=0.25, gamma=2.0):\nsuper(FocalLoss, self).__init__()\nself.alpha = alpha\nself.gamma = gamma\ndef forward(self, predictions, targets):\n# Implementation of focal loss for classification\n# ...\nreturn loss\nclass DetectionLoss(nn.Module):\ndef __init__(self, num_classes, alpha=0.25, gamma=2.0):\nsuper(DetectionLoss, self).__init__()\nself.num_classes = num_classes\nself.focal_loss = FocalLoss(alpha, gamma)\ndef forward(self, classifications, regressions, targets):\n# Classification loss (focal loss)\ncls_loss = self.focal_loss(classifications, targets['cls_targets'])\n# Regression loss (smooth L1)\nreg_loss = F.smooth_l1_loss(regressions, targets['reg_targets'])\n# Combine losses\nreturn cls_loss + reg_loss\nclass ObjectDetector(nn.Module):\ndef __init__(self, num_classes, num_anchors):\nsuper(ObjectDetector, self).__init__()\nself.num_classes = num_classes\nself.num_anchors = num_anchors\n# Base network (feature extractor)\nself.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\nself.relu1 = nn.ReLU()\nself.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\nself.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\nself.relu2 = nn.ReLU()\nself.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\nself.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\nself.relu3 = nn.ReLU()\nself.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\nself.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\nself.relu4 = nn.ReLU()\n# Classification head\nself.cls_conv1 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\nself.cls_relu = nn.ReLU()\nself.cls_conv2 = nn.Conv2d(256, num_classes * num_anchors, kernel_size=1)\n# Bounding box regression head\nself.reg_conv1 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\nself.reg_relu = nn.ReLU()\nself.reg_conv2 = nn.Conv2d(256, 4 * num_anchors, kernel_size=1)\ndef forward(self, x):\n# Base network\nx = self.pool1(self.relu1(self.conv1(x)))\nx = self.pool2(self.relu2(self.conv2(x)))\nx = self.pool3(self.relu3(self.conv3(x)))\nx = self.relu4(self.conv4(x))\n# Get spatial dimensions for reshaping\nbatch_size, _, height, width = x.shape\n# Classification head\ncls = self.cls_relu(self.cls_conv1(x))\ncls = self.cls_conv2(cls)\ncls = cls.permute(0, 2, 3, 1).contiguous()\ncls = cls.view(batch_size, height * width * self.num_anchors, self.num_classes)\ncls = F.softmax(cls, dim=2)\n# Bounding box regression head\nreg = self.reg_relu(self.reg_conv1(x))\nreg = self.reg_conv2(reg)\nreg = reg.permute(0, 2, 3, 1).contiguous()\nreg = reg.view(batch_size, height * width * self.num_anchors, 4)\nreturn cls, reg\n# Create model\nnum_classes = 80  # For COCO dataset\nnum_anchors = 9   # Typical number of anchors per location\nmodel = ObjectDetector(num_classes, num_anchors)\ncriterion = DetectionLoss(num_classes)\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n# Training loop\nfor epoch in range(100):\nrunning_loss = 0.0\nfor inputs, targets in coco_dataloader:\n# Zero the parameter gradients\noptimizer.zero_grad()\n# Forward\nclassifications, regressions = model(inputs)\nloss = criterion(classifications, regressions, targets)\n# Backward + optimize\nloss.backward()\noptimizer.step()\nrunning_loss += loss.item()\nprint(f'Epoch {epoch+1}, Loss: {running_loss/len(coco_dataloader):.4f}')\n</code></pre>"},{"location":"examples/computer-vision/#object-detection-components-explained","title":"Object Detection Components Explained","text":"<ol> <li>Base Network: Extracts features from the input image</li> <li>Multiple Scales: Extract features at different scales for objects of different sizes</li> <li>Parallel Branches:</li> <li>Classification head: Predicts the class of objects at each anchor position</li> <li>Bounding box regression head: Predicts the coordinates of the bounding boxes</li> <li>Anchors: Predefined boxes of various sizes and aspect ratios that serve as references</li> <li>Specialized Loss Functions:</li> <li>Focal Loss: Addresses class imbalance in object detection</li> <li>Smooth L1 Loss: For bounding box regression</li> </ol>"},{"location":"examples/computer-vision/#next-steps","title":"Next Steps","text":"<p>Now that you've learned about computer vision models in Nova, you can explore:</p> <ul> <li>NLP Examples: Learn how to build models for text data</li> <li>Advanced Topics: Discover techniques like transfer learning and GANs</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install Nova and set up your development environment.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing Nova, make sure you have the following prerequisites:</p> <ul> <li>Python 3.8 or higher</li> <li>pip (Python package installer)</li> <li>PyTorch 2.0 or higher</li> </ul>"},{"location":"getting-started/installation/#installing-nova","title":"Installing Nova","text":""},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<p>The simplest way to install Nova is using pip:</p> <pre><code>pip install nova-dl\n</code></pre> <p>This will install Nova and its dependencies, including PyTorch if it's not already installed.</p>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>To install Nova from source, follow these steps:</p> <ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/nova-team/nova.git\ncd nova\n</code></pre> <ol> <li>Install the package in development mode:</li> </ol> <pre><code>pip install -e .\n</code></pre>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>To verify that Nova is installed correctly, run the following Python code:</p> <pre><code>import nova\nprint(nova.__version__)\n</code></pre> <p>You should see the version number of your Nova installation printed.</p>"},{"location":"getting-started/installation/#setting-up-your-environment","title":"Setting Up Your Environment","text":""},{"location":"getting-started/installation/#virtual-environment-recommended","title":"Virtual Environment (Recommended)","text":"<p>It's recommended to use a virtual environment for your Nova projects:</p> <pre><code>python -m venv nova-env\nsource nova-env/bin/activate  # On Windows: nova-env\\Scripts\\activate\npip install nova-dl\n</code></pre>"},{"location":"getting-started/installation/#jupyter-notebook-setup","title":"Jupyter Notebook Setup","text":"<p>If you're using Jupyter Notebooks, install the following packages:</p> <pre><code>pip install jupyter ipykernel\npython -m ipykernel install --user --name=nova-env --display-name=\"Python (Nova)\"\n</code></pre> <p>Then start Jupyter and select the \"Python (Nova)\" kernel.</p>"},{"location":"getting-started/installation/#installing-optional-dependencies","title":"Installing Optional Dependencies","text":"<p>Depending on your needs, you might want to install additional packages:</p> <pre><code># For visualization support\npip install matplotlib seaborn\n\n# For data manipulation\npip install pandas numpy\n\n# For advanced deep learning models\npip install torchvision torchaudio\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-installation-issues","title":"Common Installation Issues","text":""},{"location":"getting-started/installation/#pytorch-installation","title":"PyTorch Installation","text":"<p>If you encounter issues with PyTorch, visit the official PyTorch installation guide for platform-specific installation instructions.</p>"},{"location":"getting-started/installation/#cuda-compatibility","title":"CUDA Compatibility","text":"<p>For GPU acceleration, ensure your CUDA version is compatible with your PyTorch version. You can check your CUDA version with:</p> <pre><code>nvidia-smi\n</code></pre> <p>And install a compatible PyTorch version using the instructions on the PyTorch website.</p>"},{"location":"getting-started/installation/#package-conflicts","title":"Package Conflicts","text":"<p>If you encounter package conflicts, try creating a fresh virtual environment:</p> <pre><code>python -m venv nova-clean-env\nsource nova-clean-env/bin/activate\npip install nova-dl\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that you have Nova installed, you can proceed to the Quick Start Guide to create your first model using Nova.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start Guide","text":"<p>This guide will help you get started with Nova quickly. You'll learn how to create a simple neural network, translate it to PyTorch code, and run it.</p>"},{"location":"getting-started/quick-start/#basic-usage","title":"Basic Usage","text":"<p>The simplest way to use Nova is through its Python API:</p> <pre><code>from nova import NovaInterpreter\n# Create an interpreter instance\ninterpreter = NovaInterpreter()\n# Define a model using Nova language\nnova_code = \"\"\"\ncreate processing pipeline simple_model:\n    add transformation stage fully_connected with 10 inputs and 5 outputs\n    apply relu activation\n    add transformation stage fully_connected with 5 inputs and 1 outputs\n    apply sigmoid activation\n\"\"\"\n# Translate Nova code to PyTorch code\npytorch_code = interpreter.translate(nova_code)\n# Print the generated code\nprint(pytorch_code)\n# Execute the generated code\nexec(pytorch_code)\n# Now you can use the model\nprint(model)\n</code></pre>"},{"location":"getting-started/quick-start/#complete-example-mnist-digit-classification","title":"Complete Example: MNIST Digit Classification","text":"<p>Let's create a simple digit classifier for the MNIST dataset using Nova:</p> <pre><code>from nova import NovaInterpreter\nimport torch\n# Create an interpreter instance\ninterpreter = NovaInterpreter()\n# Define a complete model with data loading and training\nnova_code = \"\"\"\n# Load MNIST dataset\nload data collection mnist from torchvision.datasets with:\n    apply normalization with mean 0.1307 and deviation 0.3081\n    convert to feature grid\n# Prepare data streams\nprepare data stream train_stream from mnist.train with batch size 64 and shuffle enabled\nprepare data stream test_stream from mnist.test with batch size 1000\n# Create a neural network for digit classification\ncreate processing pipeline digit_classifier:\n    add transformation stage fully_connected with 784 inputs and 128 outputs\n    apply relu activation\n    add transformation stage fully_connected with 128 inputs and 10 outputs\n# Train the model\ntrain digit_classifier on train_stream:\n    measure error using cross_entropy\n    improve using adam with learning rate 0.001\n    repeat for 3 learning cycles\n# Evaluate the model\nevaluate digit_classifier on test_stream:\n    measure accuracy\n    report results\n\"\"\"\n# Translate and execute\npytorch_code = interpreter.translate(nova_code)\nexec(pytorch_code)\n</code></pre>"},{"location":"getting-started/quick-start/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"getting-started/quick-start/#1-creating-a-model","title":"1. Creating a Model","text":"<p>In Nova, models are called \"processing pipelines\" and are defined with a clear, natural syntax:</p> <pre><code>create processing pipeline model_name:\n    add transformation stage fully_connected with 784 inputs and 128 outputs\n    apply relu activation\n    add transformation stage fully_connected with 128 inputs and 10 outputs\n</code></pre> <p>This creates a simple neural network with two fully connected layers and ReLU activation.</p>"},{"location":"getting-started/quick-start/#2-loading-data","title":"2. Loading Data","text":"<p>Nova provides intuitive syntax for loading and preparing data:</p> <pre><code>load data collection mnist from torchvision.datasets with:\n    apply normalization with mean 0.1307 and deviation 0.3081\n\nprepare data stream train_stream from mnist.train with batch size 64 and shuffle enabled\n</code></pre>"},{"location":"getting-started/quick-start/#3-training-the-model","title":"3. Training the Model","text":"<p>Training is expressed in terms of \"error measures\" (loss functions) and \"improvement strategies\" (optimizers):</p> <pre><code>train model_name on train_stream:\n    measure error using cross_entropy\n    improve using adam with learning rate 0.001\n    repeat for 5 learning cycles\n</code></pre>"},{"location":"getting-started/quick-start/#4-evaluating-the-model","title":"4. Evaluating the Model","text":"<p>Evaluation is similarly expressed in intuitive terms:</p> <pre><code>evaluate model_name on test_stream:\n    measure accuracy\n    report results\n</code></pre>"},{"location":"getting-started/quick-start/#understanding-the-translation","title":"Understanding the Translation","text":"<p>Nova translates natural language instructions into proper PyTorch code. You can examine the translation with:</p> <pre><code># Get an explanation of the translation\nexplanation = interpreter.explain_translation(nova_code, pytorch_code)\nprint(explanation)\n</code></pre> <p>This helps you understand how your Nova code maps to PyTorch constructs, aiding the learning process.</p>"},{"location":"getting-started/quick-start/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quick-start/#convolutional-neural-networks","title":"Convolutional Neural Networks","text":"<pre><code>create image processing pipeline cnn_model:\n    add feature detector with 1 input channels, 32 output channels and 3x3 filter size\n    apply relu activation\n    add downsampling using max method with size 2x2\n    add feature detector with 32 input channels, 64 output channels and 3x3 filter size\n    apply relu activation\n    add downsampling using max method with size 2x2\n    flatten features\n    add transformation stage fully_connected with 1600 inputs and 10 outputs\n</code></pre>"},{"location":"getting-started/quick-start/#recurrent-neural-networks","title":"Recurrent Neural Networks","text":"<pre><code>create sequence processing pipeline rnn_model:\n    add memory cell lstm with 50 inputs and 100 hidden size\n    add transformation stage fully_connected with 100 inputs and 10 outputs\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you've created your first model with Nova, you can explore more complex architectures and features:</p> <ul> <li>Language Guide: Learn the full Nova syntax</li> <li>Examples: Explore more practical examples</li> <li>Core Concepts: Understand the concepts behind Nova</li> </ul>"},{"location":"language-guide/core-concepts/","title":"Core Concepts","text":"<p>This guide explains the fundamental concepts of Nova and how they map to PyTorch constructs. Understanding these core concepts will help you effectively use Nova for deep learning tasks.</p>"},{"location":"language-guide/core-concepts/#data-concepts","title":"Data Concepts","text":""},{"location":"language-guide/core-concepts/#feature-grids-tensors","title":"Feature Grids (Tensors)","text":"<p>In Nova, we use the term \"feature grid\" to represent what PyTorch calls tensors. These are multi-dimensional arrays that store numerical data.</p> Nova <pre><code>create feature grid image_data with shape 3x224x224\n</code></pre> PyTorch <pre><code>image_tensor = torch.zeros(3, 224, 224)\n</code></pre> <p>Feature grids have: - Dimensions: The number of axes (1D, 2D, 3D, etc.) - Shape: The size along each dimension - Data type: The type of values stored (floating point, integer, etc.)</p>"},{"location":"language-guide/core-concepts/#data-collections-datasets","title":"Data Collections (Datasets)","text":"<p>A \"data collection\" in Nova represents a PyTorch Dataset. It contains multiple samples used for training or evaluation.</p> Nova <pre><code>load data collection mnist from torchvision.datasets\n</code></pre> PyTorch <pre><code>from torchvision.datasets import MNIST\nmnist_dataset = MNIST(root='./data', download=True, transform=transforms.ToTensor())\n</code></pre>"},{"location":"language-guide/core-concepts/#data-streams-dataloaders","title":"Data Streams (DataLoaders)","text":"<p>A \"data stream\" represents a PyTorch DataLoader, which provides batches of data from a dataset.</p> Nova <pre><code>prepare data stream from mnist with batch size 32 and shuffle enabled\n</code></pre> PyTorch <pre><code>mnist_dataloader = DataLoader(mnist_dataset, batch_size=32, shuffle=True)\n</code></pre>"},{"location":"language-guide/core-concepts/#model-architecture-concepts","title":"Model Architecture Concepts","text":""},{"location":"language-guide/core-concepts/#processing-pipelines-neural-networks","title":"Processing Pipelines (Neural Networks)","text":"<p>A \"processing pipeline\" in Nova represents a complete neural network model (PyTorch's nn.Module).</p> Nova <pre><code>create processing pipeline classifier:\n    add transformation stage fully_connected with 784 inputs and 128 outputs\n    apply relu activation\n    add transformation stage fully_connected with 128 inputs and 10 outputs\n</code></pre> PyTorch <pre><code>class Classifier(nn.Module):\ndef __init__(self):\nsuper(Classifier, self).__init__()\nself.fc1 = nn.Linear(784, 128)\nself.relu = nn.ReLU()\nself.fc2 = nn.Linear(128, 10)\ndef forward(self, x):\nx = self.fc1(x)\nx = self.relu(x)\nx = self.fc2(x)\nreturn x\n</code></pre>"},{"location":"language-guide/core-concepts/#transformation-stages-layers","title":"Transformation Stages (Layers)","text":"<p>A \"transformation stage\" represents a layer in a neural network. Different types include:</p>"},{"location":"language-guide/core-concepts/#1-fully-connected-linear-layer","title":"1. Fully Connected (Linear Layer)","text":"Nova <pre><code>add transformation stage fully_connected with 784 inputs and 128 outputs\n</code></pre> PyTorch <pre><code>nn.Linear(784, 128)\n</code></pre>"},{"location":"language-guide/core-concepts/#2-feature-detector-convolutional-layer","title":"2. Feature Detector (Convolutional Layer)","text":"Nova <pre><code>add feature detector with 3 input channels, 16 output channels and 3x3 filter size\n</code></pre> PyTorch <pre><code>nn.Conv2d(3, 16, kernel_size=3, padding=1)\n</code></pre>"},{"location":"language-guide/core-concepts/#3-downsampling-pooling-layer","title":"3. Downsampling (Pooling Layer)","text":"Nova <pre><code>add downsampling using max method with size 2x2\n</code></pre> PyTorch <pre><code>nn.MaxPool2d(kernel_size=2, stride=2)\n</code></pre>"},{"location":"language-guide/core-concepts/#4-memory-cell-recurrent-layer","title":"4. Memory Cell (Recurrent Layer)","text":"Nova <pre><code>add memory cell lstm with 128 inputs and 256 hidden size\n</code></pre> PyTorch <pre><code>nn.LSTM(input_size=128, hidden_size=256)\n</code></pre>"},{"location":"language-guide/core-concepts/#activation-patterns-activation-functions","title":"Activation Patterns (Activation Functions)","text":"<p>\"Activation patterns\" represent activation functions that introduce non-linearity:</p> Nova <pre><code>apply relu activation\napply sigmoid activation\napply tanh activation\n</code></pre> PyTorch <pre><code>nn.ReLU()\nnn.Sigmoid()\nnn.Tanh()\n</code></pre>"},{"location":"language-guide/core-concepts/#training-concepts","title":"Training Concepts","text":""},{"location":"language-guide/core-concepts/#error-measures-loss-functions","title":"Error Measures (Loss Functions)","text":"<p>An \"error measure\" quantifies how far the model's predictions are from the true values:</p> Nova <pre><code>measure error using mean_squared_error\nmeasure error using cross_entropy\n</code></pre> PyTorch <pre><code>nn.MSELoss()\nnn.CrossEntropyLoss()\n</code></pre>"},{"location":"language-guide/core-concepts/#improvement-strategies-optimizers","title":"Improvement Strategies (Optimizers)","text":"<p>An \"improvement strategy\" defines how to update model parameters:</p> Nova <pre><code>improve using gradient_descent with learning rate 0.01\nimprove using adam with learning rate 0.001\n</code></pre> PyTorch <pre><code>optim.SGD(model.parameters(), lr=0.01)\noptim.Adam(model.parameters(), lr=0.001)\n</code></pre>"},{"location":"language-guide/core-concepts/#learning-cycles-epochs","title":"Learning Cycles (Epochs)","text":"<p>A \"learning cycle\" represents one complete pass through the training dataset:</p> Nova <pre><code>repeat for 10 learning cycles\n</code></pre> PyTorch <pre><code>for epoch in range(10):\n</code></pre>"},{"location":"language-guide/core-concepts/#improvement-steps-backpropagation-update","title":"Improvement Steps (Backpropagation &amp; Update)","text":"<p>An \"improvement step\" represents the process of computing gradients and updating parameters:</p> Nova <pre><code># This happens implicitly in Nova's training syntax\n</code></pre> PyTorch <pre><code>optimizer.zero_grad()\nloss.backward()\noptimizer.step()\n</code></pre>"},{"location":"language-guide/core-concepts/#regularization-concepts","title":"Regularization Concepts","text":""},{"location":"language-guide/core-concepts/#weight-decay","title":"Weight Decay","text":"Nova <pre><code>improve using adam with learning rate 0.001 and weight decay 0.0001\n</code></pre> PyTorch <pre><code>optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n</code></pre>"},{"location":"language-guide/core-concepts/#dropout","title":"Dropout","text":"Nova <pre><code>add dropout with rate 0.5\n</code></pre> PyTorch <pre><code>nn.Dropout(0.5)\n</code></pre>"},{"location":"language-guide/core-concepts/#batch-normalization","title":"Batch Normalization","text":"Nova <pre><code>add batch normalization with 128 features\n</code></pre> PyTorch <pre><code>nn.BatchNorm1d(128)\n</code></pre>"},{"location":"language-guide/core-concepts/#evaluation-and-inference-concepts","title":"Evaluation and Inference Concepts","text":""},{"location":"language-guide/core-concepts/#performance-metrics","title":"Performance Metrics","text":"Nova <pre><code>evaluate classifier on test_stream:\n    measure accuracy, precision, and recall\n    report results\n</code></pre> PyTorch <pre><code>def evaluate(model, test_loader):\n# Implementation of accuracy, precision, recall calculations\npass\n</code></pre>"},{"location":"language-guide/core-concepts/#making-predictions","title":"Making Predictions","text":"Nova <pre><code>use classifier to process new_data\n</code></pre> PyTorch <pre><code>model(new_data)\n</code></pre>"},{"location":"language-guide/core-concepts/#model-persistence-concepts","title":"Model Persistence Concepts","text":""},{"location":"language-guide/core-concepts/#saving-models","title":"Saving Models","text":"Nova <pre><code>save classifier to \"models/classifier.pth\"\n</code></pre> PyTorch <pre><code>torch.save(model.state_dict(), \"models/classifier.pth\")\n</code></pre>"},{"location":"language-guide/core-concepts/#loading-models","title":"Loading Models","text":"Nova <pre><code>load classifier from \"models/classifier.pth\"\n</code></pre> PyTorch <pre><code>model.load_state_dict(torch.load(\"models/classifier.pth\"))\n</code></pre>"},{"location":"language-guide/core-concepts/#concept-mapping-summary","title":"Concept Mapping Summary","text":"<p>The table below summarizes how Nova concepts map to PyTorch:</p> Domain Nova Concept PyTorch Concept Data Feature Grid Tensor Data Collection Dataset Data Stream DataLoader Model Processing Pipeline nn.Module Transformation Stage Layer (Linear, Conv2d, etc.) Feature Detector Convolutional Layer Downsampling Pooling Layer Memory Cell RNN/LSTM/GRU Connection Strengths Weights &amp; Biases Training Error Measure Loss Function Improvement Strategy Optimizer Learning Cycle Epoch Improvement Step Backpropagation &amp; Update Evaluation Performance Metric Accuracy, Precision, etc."},{"location":"language-guide/core-concepts/#next-steps","title":"Next Steps","text":"<p>Now that you understand the core concepts of Nova, proceed to the Syntax guide to learn how to combine these concepts into complete programs.</p>"},{"location":"language-guide/overview/","title":"Nova Language Overview","text":"<p>Nova is a natural language interface designed to make deep learning more accessible and intuitive. This guide provides an overview of the Nova language, its design philosophy, and its key components.</p>"},{"location":"language-guide/overview/#design-philosophy","title":"Design Philosophy","text":"<p>Nova is built on several core principles:</p>"},{"location":"language-guide/overview/#1-intuitive-terminology","title":"1. Intuitive Terminology","text":"<p>Nova replaces technical jargon with intuitive terms that better reflect what's happening conceptually:</p> PyTorch Term Nova Term Rationale Neural Network Processing Pipeline Emphasizes data transformation aspect Layer Transformation Stage Clarifies the role in data processing Weights Connection Strengths Describes what they represent Loss Function Error Measure Explains their purpose Optimizer Improvement Strategy Describes their role Epoch Learning Cycle Emphasizes the iterative nature"},{"location":"language-guide/overview/#2-pipeline-metaphor","title":"2. Pipeline Metaphor","text":"<p>Nova models neural networks as data transformation pipelines, a concept familiar to most programmers who have worked with:</p> <ul> <li>ETL (Extract, Transform, Load) processes</li> <li>Unix pipes</li> <li>Functional programming chains</li> <li>Data processing workflows</li> </ul> <p>This metaphor makes it easier to conceptualize how data flows through neural networks.</p>"},{"location":"language-guide/overview/#3-descriptive-operations","title":"3. Descriptive Operations","text":"<p>Nova operations are designed to be self-documenting, making code more readable and maintainable:</p> <pre><code># PyTorch\nmodel = nn.Sequential(\n    nn.Linear(784, 128),\n    nn.ReLU(),\n    nn.Linear(128, 10)\n)\n\n# Nova\ncreate processing pipeline model:\n    add transformation stage fully_connected with 784 inputs and 128 outputs\n    apply relu activation\n    add transformation stage fully_connected with 128 inputs and 10 outputs\n</code></pre>"},{"location":"language-guide/overview/#4-educational-bridge","title":"4. Educational Bridge","text":"<p>Nova serves as a bridge between intuitive understanding and technical implementation. By showing the translation from Nova to PyTorch, users can gradually learn the underlying PyTorch concepts while using a more accessible syntax.</p>"},{"location":"language-guide/overview/#language-structure","title":"Language Structure","text":"<p>The Nova language is structured around several key components:</p>"},{"location":"language-guide/overview/#1-data-components","title":"1. Data Components","text":"<ul> <li>Data collections (datasets)</li> <li>Data streams (dataloaders)</li> <li>Feature grids (tensors)</li> <li>Transformations (preprocessing)</li> </ul>"},{"location":"language-guide/overview/#2-model-components","title":"2. Model Components","text":"<ul> <li>Processing pipelines (neural networks)</li> <li>Transformation stages (layers)</li> <li>Connection strengths (weights)</li> <li>Activation patterns (activation functions)</li> </ul>"},{"location":"language-guide/overview/#3-training-components","title":"3. Training Components","text":"<ul> <li>Error measures (loss functions)</li> <li>Improvement strategies (optimizers)</li> <li>Learning cycles (epochs)</li> <li>Performance metrics (evaluation metrics)</li> </ul>"},{"location":"language-guide/overview/#nova-vs-other-approaches","title":"Nova vs. Other Approaches","text":""},{"location":"language-guide/overview/#nova-vs-high-level-libraries","title":"Nova vs. High-Level Libraries","text":"<p>Unlike high-level libraries that hide implementation details, Nova aims to be transparent by showing the translation to PyTorch code. This aids the learning process and allows for greater flexibility.</p>"},{"location":"language-guide/overview/#nova-vs-code-generation","title":"Nova vs. Code Generation","text":"<p>Nova is not just a code generation tool. It provides a consistent language with well-defined semantics that maps to machine learning concepts. The goal is to make the language itself intuitive and educational.</p>"},{"location":"language-guide/overview/#nova-vs-natural-language-programming","title":"Nova vs. Natural Language Programming","text":"<p>Nova has more structure than free-form natural language programming, providing a balance between flexibility and consistency. This makes it more reliable for machine learning tasks while still being intuitive.</p>"},{"location":"language-guide/overview/#learning-path","title":"Learning Path","text":"<p>To get the most out of Nova, we recommend the following learning path:</p> <ol> <li>Start with Core Concepts to understand the fundamental components of Nova</li> <li>Learn the Syntax to see how to express different operations</li> <li>Explore the Translation Process to understand how Nova maps to PyTorch</li> <li>Try the Examples to see Nova in action</li> </ol>"},{"location":"language-guide/overview/#next-steps","title":"Next Steps","text":"<p>Continue to the Core Concepts guide to learn about the fundamental components of Nova.</p>"},{"location":"language-guide/syntax/","title":"Nova Syntax Guide","text":"<p>This guide provides a comprehensive reference for Nova's syntax, covering all the major operations and constructs available in the language.</p>"},{"location":"language-guide/syntax/#basic-syntax-rules","title":"Basic Syntax Rules","text":"<p>Nova's syntax follows a few general principles:</p> <ol> <li>Natural language constructs: Instructions and operations are written in a way that reads like natural language</li> <li>Hierarchical structure: Operations can be nested under higher-level constructs</li> <li>Explicit parameters: Parameter names are explicitly stated for clarity</li> <li>Consistent patterns: Similar operations follow similar syntax patterns</li> </ol>"},{"location":"language-guide/syntax/#data-operations","title":"Data Operations","text":""},{"location":"language-guide/syntax/#creating-tensors","title":"Creating Tensors","text":"<pre><code># Create an empty tensor with a specified shape\ncreate feature grid [name] with shape [dimensions]\n\n# Create a tensor from existing data\ncreate feature grid [name] from [data]\n\n# Examples\ncreate feature grid input_data with shape 3x224x224\ncreate feature grid weights from numpy_array\n</code></pre>"},{"location":"language-guide/syntax/#loading-datasets","title":"Loading Datasets","text":"<pre><code># Load a dataset from a standard source\nload data collection [name] from [source]\n\n# Create a dataset from files or arrays\ncreate data collection [name] from [arrays/files/etc]\n\n# Examples\nload data collection mnist from torchvision.datasets\ncreate data collection custom_dataset from \"data/images/\"\n</code></pre>"},{"location":"language-guide/syntax/#data-preparation","title":"Data Preparation","text":"<pre><code># Split a dataset for training and testing\nsplit collection into [training_percent]% training and [testing_percent]% testing\n\n# Create a data loader from a dataset\nprepare data stream from [collection] with batch size [number]\n\n# Optional parameters for data streams\nprepare data stream from [collection] with batch size [number] and shuffle enabled\n\n# Examples\nsplit mnist into 80% training and 20% testing\nprepare data stream train_stream from mnist.train with batch size 64 and shuffle enabled\n</code></pre>"},{"location":"language-guide/syntax/#data-transformations","title":"Data Transformations","text":"<pre><code># Define transformations for data\ndefine transformations for [collection]:\n    [transformation operations...]\n\n# Common transformation operations\nresize images to [dimensions]\nnormalize values using mean [mean_values] and deviation [std_values]\nrandom horizontal flip with probability [probability]\n\n# Examples\ndefine transformations for cifar10:\n    resize images to 32x32\n    normalize values using mean [0.4914, 0.4822, 0.4465] and deviation [0.2470, 0.2435, 0.2616]\n    random horizontal flip with probability 0.5\n</code></pre>"},{"location":"language-guide/syntax/#model-definition","title":"Model Definition","text":""},{"location":"language-guide/syntax/#basic-model-creation","title":"Basic Model Creation","text":"<pre><code># Create a basic neural network model\ncreate processing pipeline [name]:\n    [layer definitions...]\n\n# Examples\ncreate processing pipeline digit_recognizer:\n    add transformation stage fully_connected with 784 inputs and 128 outputs\n    apply relu activation\n    add transformation stage fully_connected with 128 inputs and 10 outputs\n</code></pre>"},{"location":"language-guide/syntax/#fully-connected-layers","title":"Fully Connected Layers","text":"<pre><code># Add a fully connected (linear) layer\nadd transformation stage fully_connected with [inputs] inputs and [outputs] outputs\n\n# Examples\nadd transformation stage fully_connected with 784 inputs and 128 outputs\nadd transformation stage fully_connected with 128 inputs and 10 outputs\n</code></pre>"},{"location":"language-guide/syntax/#convolutional-layers","title":"Convolutional Layers","text":"<pre><code># Add a convolutional layer\nadd feature detector with [in_channels] input channels, [out_channels] output channels and [size]x[size] filter size\n\n# With optional parameters\nadd feature detector with [in_channels] input channels, [out_channels] output channels, [size]x[size] filter size and padding [padding]\n\n# Examples\nadd feature detector with 3 input channels, 16 output channels and 3x3 filter size\nadd feature detector with 16 input channels, 32 output channels, 3x3 filter size and padding same\n</code></pre>"},{"location":"language-guide/syntax/#pooling-layers","title":"Pooling Layers","text":"<pre><code># Add a pooling layer\nadd downsampling using [method] method with size [size]x[size]\n\n# Examples\nadd downsampling using max method with size 2x2\nadd downsampling using average method with size 3x3\n</code></pre>"},{"location":"language-guide/syntax/#recurrent-layers","title":"Recurrent Layers","text":"<pre><code># Add a recurrent layer\nadd memory cell [type] with [inputs] inputs and [hidden_size] hidden size\n\n# Examples\nadd memory cell lstm with 50 inputs and 100 hidden size\nadd memory cell gru with 64 inputs and 128 hidden size\n</code></pre>"},{"location":"language-guide/syntax/#activation-functions","title":"Activation Functions","text":"<pre><code># Apply an activation function\napply [function_name] activation\n\n# Examples\napply relu activation\napply sigmoid activation\napply tanh activation\napply leaky_relu activation\n</code></pre>"},{"location":"language-guide/syntax/#regularization-layers","title":"Regularization Layers","text":"<pre><code># Add dropout\nadd dropout with rate [rate]\n\n# Add batch normalization\nadd batch normalization with [features] features\n\n# Examples\nadd dropout with rate 0.5\nadd batch normalization with 128 features\n</code></pre>"},{"location":"language-guide/syntax/#special-operations","title":"Special Operations","text":"<pre><code># Flatten a tensor (e.g., after convolutional layers)\nflatten features\n\n# Reshape a tensor\nreshape features to [dimensions]\n\n# Examples\nflatten features\nreshape features to 1x28x28\n</code></pre>"},{"location":"language-guide/syntax/#training-process","title":"Training Process","text":""},{"location":"language-guide/syntax/#basic-training","title":"Basic Training","text":"<pre><code># Train a model on a dataset\ntrain [model_name] on [data_stream]:\n    [training parameters...]\n\n# Examples\ntrain digit_recognizer on mnist_train_stream:\n    measure error using cross_entropy\n    improve using adam with learning rate 0.001\n    repeat for 10 learning cycles\n</code></pre>"},{"location":"language-guide/syntax/#error-measures-loss-functions","title":"Error Measures (Loss Functions)","text":"<pre><code># Specify a loss function\nmeasure error using [error_function]\n\n# Examples\nmeasure error using mean_squared_error\nmeasure error using cross_entropy\nmeasure error using binary_cross_entropy\n</code></pre>"},{"location":"language-guide/syntax/#improvement-strategies-optimizers","title":"Improvement Strategies (Optimizers)","text":"<pre><code># Specify an optimizer\nimprove using [strategy] with learning rate [rate]\n\n# With additional parameters\nimprove using [strategy] with learning rate [rate] and [parameter] [value]\n\n# Examples\nimprove using gradient_descent with learning rate 0.01\nimprove using adam with learning rate 0.001\nimprove using adam with learning rate 0.001 and weight decay 0.0001\n</code></pre>"},{"location":"language-guide/syntax/#learning-cycles-epochs","title":"Learning Cycles (Epochs)","text":"<pre><code># Specify the number of training epochs\nrepeat for [cycles] learning cycles\n\n# Examples\nrepeat for 10 learning cycles\n</code></pre>"},{"location":"language-guide/syntax/#advanced-training-options","title":"Advanced Training Options","text":"<pre><code># Learning rate scheduling\nreduce learning rate by factor [factor] when plateau in validation loss for [patience] cycles\n\n# Early stopping\nstop early if no improvement for [patience] cycles\n\n# Gradient clipping\nclip gradients to maximum [max_norm]\n\n# Progress reporting\nprint progress every [interval] batches\n\n# Examples\nreduce learning rate by factor 0.1 when plateau in validation loss for 5 cycles\nstop early if no improvement for 10 cycles\nclip gradients to maximum 5.0\nprint progress every 100 batches\n</code></pre>"},{"location":"language-guide/syntax/#evaluation-and-inference","title":"Evaluation and Inference","text":""},{"location":"language-guide/syntax/#model-evaluation","title":"Model Evaluation","text":"<pre><code># Evaluate a model on a test dataset\nevaluate [model_name] on [test_stream]:\n    [evaluation parameters...]\n\n# Examples\nevaluate digit_recognizer on mnist_test_stream:\n    measure accuracy\n    report results\n</code></pre>"},{"location":"language-guide/syntax/#performance-metrics","title":"Performance Metrics","text":"<pre><code># Specify which metrics to calculate\nmeasure [metrics...]\n\n# Examples\nmeasure accuracy\nmeasure accuracy, precision, and recall\nmeasure accuracy, f1 score\n</code></pre>"},{"location":"language-guide/syntax/#making-predictions","title":"Making Predictions","text":"<pre><code># Use a model for inference\nuse [model_name] to process [input_data]\n\n# Get predictions from a model\nget predictions from [model_name] for [input_data]\n\n# Examples\nuse digit_recognizer to process test_image\nget predictions from classifier for batch_data\n</code></pre>"},{"location":"language-guide/syntax/#model-persistence","title":"Model Persistence","text":""},{"location":"language-guide/syntax/#saving-models","title":"Saving Models","text":"<pre><code># Save a model to a file\nsave [model_name] to [filepath]\n\n# Save only model parameters\nsave [model_name] parameters to [filepath]\n\n# Examples\nsave digit_recognizer to \"models/digit_recognizer.pth\"\nsave classifier parameters to \"checkpoints/weights.pth\"\n</code></pre>"},{"location":"language-guide/syntax/#loading-models","title":"Loading Models","text":"<pre><code># Load a model from a file\nload [model_name] from [filepath]\n\n# Load parameters into a model\nload parameters into [model_name] from [filepath]\n\n# Examples\nload digit_recognizer from \"models/digit_recognizer.pth\"\nload parameters into classifier from \"checkpoints/weights.pth\"\n</code></pre>"},{"location":"language-guide/syntax/#advanced-model-architectures","title":"Advanced Model Architectures","text":""},{"location":"language-guide/syntax/#residual-connections","title":"Residual Connections","text":"<pre><code># Add a residual block\nadd residual block with [in_channels] input channels, [out_channels] output channels\n\n# Examples\nadd residual block with 64 input channels, 64 output channels\n</code></pre>"},{"location":"language-guide/syntax/#attention-mechanisms","title":"Attention Mechanisms","text":"<pre><code># Add an attention mechanism\nadd attention mechanism with [query_dim] query dimension and [key_dim] key dimension\n\n# Examples\nadd attention mechanism with 512 query dimension and 512 key dimension\n</code></pre>"},{"location":"language-guide/syntax/#transfer-learning","title":"Transfer Learning","text":"<pre><code># Load a pretrained model\nload pretrained [model_type] model\n\n# Freeze layers\nfreeze all layers except [layer_names]\n\n# Replace layers\nreplace final layer with transformation stage fully_connected with [inputs] inputs and [outputs] outputs\n\n# Examples\nload pretrained resnet18 model\nfreeze all layers except final layer\nreplace final layer with transformation stage fully_connected with 512 inputs and 10 outputs\n</code></pre>"},{"location":"language-guide/syntax/#comments-and-documentation","title":"Comments and Documentation","text":"<pre><code># Single line comments start with a hash symbol\n\n# Multiple line comments\n# can be written like\n# this\n</code></pre>"},{"location":"language-guide/syntax/#next-steps","title":"Next Steps","text":"<p>Now that you understand Nova's syntax, proceed to the Translation Process guide to learn how Nova code is translated to PyTorch.</p>"},{"location":"language-guide/translation-process/","title":"Translation Process","text":"<p>This guide explains how Nova code is translated into executable PyTorch code. Understanding this process helps you bridge the gap between Nova's intuitive syntax and PyTorch's implementation details.</p>"},{"location":"language-guide/translation-process/#overview-of-the-translation-process","title":"Overview of the Translation Process","text":"<p>The Nova interpreter follows these steps to translate Nova code to PyTorch:</p> <ol> <li>Parsing: Break down Nova code into components and operations</li> <li>Analysis: Identify the relationships between components</li> <li>Code Generation: Create equivalent PyTorch code</li> <li>Explanation: Generate explanations of the translation</li> </ol>"},{"location":"language-guide/translation-process/#step-1-parsing","title":"Step 1: Parsing","text":"<p>During parsing, the Nova interpreter identifies key constructs in your code:</p> <ul> <li>Model definitions</li> <li>Layer specifications</li> <li>Training parameters</li> <li>Data operations</li> </ul> <p>For example, given this Nova code:</p> <pre><code>create processing pipeline simple_model:\n    add transformation stage fully_connected with 10 inputs and 5 outputs\n    apply relu activation\n</code></pre> <p>The parser identifies: - A model named \"simple_model\" - A fully connected layer with 10 inputs and 5 outputs - A ReLU activation function</p>"},{"location":"language-guide/translation-process/#step-2-analysis","title":"Step 2: Analysis","text":"<p>During analysis, the interpreter:</p> <ul> <li>Resolves dependencies between components</li> <li>Validates the coherence of the model architecture</li> <li>Determines the proper sequence of operations</li> <li>Identifies implied operations that need to be made explicit in PyTorch</li> </ul> <p>For instance, it validates that the activation function follows a layer that produces compatible outputs.</p>"},{"location":"language-guide/translation-process/#step-3-code-generation","title":"Step 3: Code Generation","text":"<p>The code generation process creates equivalent PyTorch code for each Nova component:</p>"},{"location":"language-guide/translation-process/#model-generation","title":"Model Generation","text":"<p>Nova models (\"processing pipelines\") are translated to PyTorch <code>nn.Module</code> classes:</p> Nova <pre><code>create processing pipeline simple_model:\n    add transformation stage fully_connected with 10 inputs and 5 outputs\n    apply relu activation\n    add transformation stage fully_connected with 5 inputs and 1 outputs\n</code></pre> PyTorch <pre><code>class SimpleModel(nn.Module):\ndef __init__(self):\nsuper(SimpleModel, self).__init__()\nself.fc1 = nn.Linear(10, 5)\nself.relu = nn.ReLU()\nself.fc2 = nn.Linear(5, 1)\ndef forward(self, x):\nx = self.fc1(x)\nx = self.relu(x)\nx = self.fc2(x)\nreturn x\nmodel = SimpleModel()\n</code></pre> <p>The translation process:</p> <ol> <li>Creates a class that inherits from <code>nn.Module</code></li> <li>Initializes layers in <code>__init__</code></li> <li>Defines data flow in the <code>forward</code> method</li> <li>Instantiates the model</li> </ol>"},{"location":"language-guide/translation-process/#training-code-generation","title":"Training Code Generation","text":"<p>Nova training instructions translate to PyTorch training loops:</p> Nova <pre><code>train simple_model on data_stream:\n    measure error using mean_squared_error\n    improve using adam with learning rate 0.001\n    repeat for 5 learning cycles\n</code></pre> PyTorch <pre><code>criterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n# Training loop\nfor epoch in range(5):\nfor inputs, targets in data_stream:\n# Zero the parameter gradients\noptimizer.zero_grad()\n# Forward pass\noutputs = model(inputs)\nloss = criterion(outputs, targets)\n# Backward pass and optimize\nloss.backward()\noptimizer.step()\n</code></pre> <p>The translation process:</p> <ol> <li>Sets up the loss function based on the specified error measure</li> <li>Creates an optimizer with the specified parameters</li> <li>Generates a training loop with the specified number of epochs</li> <li>Adds the necessary gradient calculation and parameter update steps</li> </ol>"},{"location":"language-guide/translation-process/#data-processing-generation","title":"Data Processing Generation","text":"<p>Nova data operations translate to PyTorch data handling code:</p> Nova <pre><code>load data collection mnist from torchvision.datasets with:\n    apply normalization with mean 0.1307 and deviation 0.3081\n\nprepare data stream from mnist with batch size 32 and shuffle enabled\n</code></pre> PyTorch <pre><code>transform = transforms.Compose([\ntransforms.ToTensor(),\ntransforms.Normalize((0.1307,), (0.3081,))\n])\nmnist_dataset = datasets.MNIST('./data', download=True, transform=transform)\ndata_loader = DataLoader(mnist_dataset, batch_size=32, shuffle=True)\n</code></pre>"},{"location":"language-guide/translation-process/#step-4-explanation-generation","title":"Step 4: Explanation Generation","text":"<p>The Nova interpreter can generate explanations of the translation to help you understand the mapping:</p> <pre><code>from nova import NovaInterpreter\ninterpreter = NovaInterpreter()\npytorch_code = interpreter.translate(nova_code)\nexplanation = interpreter.explain_translation(nova_code, pytorch_code)\nprint(explanation)\n</code></pre> <p>Example explanation output:</p> <pre><code>Translation Explanation:\n1. Created a PyTorch model class called 'SimpleModel' that inherits from nn.Module\n2. Added a fully connected layer (nn.Linear) with 10 inputs and 5 outputs\n3. Added a ReLU activation function\n4. Added another fully connected layer with 5 inputs and 1 output\n5. Defined the forward method to connect these layers in sequence\n6. Created loss function (MSELoss) based on the 'mean_squared_error' error measure\n7. Created Adam optimizer with learning rate 0.001\n8. Generated a training loop with 5 epochs\n</code></pre>"},{"location":"language-guide/translation-process/#translation-examples","title":"Translation Examples","text":""},{"location":"language-guide/translation-process/#cnn-translation","title":"CNN Translation","text":"Nova <pre><code>create image processing pipeline cnn_model:\n    add feature detector with 1 input channels, 32 output channels and 3x3 filter size\n    apply relu activation\n    add downsampling using max method with size 2x2\n    flatten features\n    add transformation stage fully_connected with 6272 inputs and 10 outputs\n</code></pre> PyTorch <pre><code>class CnnModel(nn.Module):\ndef __init__(self):\nsuper(CnnModel, self).__init__()\nself.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\nself.relu = nn.ReLU()\nself.pool = nn.MaxPool2d(kernel_size=2, stride=2)\nself.fc = nn.Linear(6272, 10)\ndef forward(self, x):\nx = self.conv1(x)\nx = self.relu(x)\nx = self.pool(x)\nx = x.view(x.size(0), -1)  # Flatten\nx = self.fc(x)\nreturn x\nmodel = CnnModel()\n</code></pre>"},{"location":"language-guide/translation-process/#rnn-translation","title":"RNN Translation","text":"Nova <pre><code>create sequence processing pipeline rnn_model:\n    add memory cell lstm with 50 inputs and 100 hidden size\n    add transformation stage fully_connected with 100 inputs and 10 outputs\n</code></pre> PyTorch <pre><code>class RnnModel(nn.Module):\ndef __init__(self):\nsuper(RnnModel, self).__init__()\nself.lstm = nn.LSTM(input_size=50, hidden_size=100, batch_first=True)\nself.fc = nn.Linear(100, 10)\ndef forward(self, x):\n# LSTM returns output and hidden state\noutput, _ = self.lstm(x)\n# Use the output from the last time step\nx = output[:, -1, :]\nx = self.fc(x)\nreturn x\nmodel = RnnModel()\n</code></pre>"},{"location":"language-guide/translation-process/#advanced-translation-considerations","title":"Advanced Translation Considerations","text":""},{"location":"language-guide/translation-process/#handling-complex-architectures","title":"Handling Complex Architectures","text":"<p>For more complex architectures, the Nova interpreter:</p> <ol> <li>Analyzes model connectivity: Determines how components connect to each other</li> <li>Resolves dimension conflicts: Inserts necessary reshaping operations</li> <li>Generates appropriate forward logic: Creates the proper sequence in the forward method</li> </ol>"},{"location":"language-guide/translation-process/#translating-custom-behavior","title":"Translating Custom Behavior","text":"<p>For cases where the direct mapping is not obvious, the interpreter makes intelligent decisions:</p> <ol> <li>Implied operations: Adds necessary operations that are implied but not explicit</li> <li>Default values: Provides reasonable defaults for optional parameters</li> <li>Best practices: Follows PyTorch best practices in the generated code</li> </ol>"},{"location":"language-guide/translation-process/#debugging-translation-issues","title":"Debugging Translation Issues","text":"<p>If you encounter issues with translation:</p> <ol> <li>Check the explanation: Review the explanation to understand the translator's decisions</li> <li>Examine the generated code: Look for any unexpected elements</li> <li>Simplify the model: Try a simpler version to isolate the problem</li> <li>Verify dimensions: Ensure that layer dimensions are compatible</li> </ol>"},{"location":"language-guide/translation-process/#next-steps","title":"Next Steps","text":"<p>Now that you understand how Nova code is translated to PyTorch, explore the Examples section to see Nova in action with practical applications.</p>"}]}